{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_regression, f_regression, f_classif, mutual_info_classif\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "# load the dataset\n",
    "data = pd.read_csv('insa-ml-2024-regression/train.csv')\n",
    "\n",
    "data.drop(columns=['hc', 'id'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On remarque qu'il y a quelques valeurs de la colonne hcnox qui manquent (nan), lais que cette variable présente une forte correlation avec le fuel_type. On peut donc essayer de prédire les valeurs manquantes de hcnox en fonction du fuel_type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_hcnox_by_fuel_type = data.groupby('fuel_type')['hcnox'].median()\n",
    "\n",
    "def fill_hcnox(row):\n",
    "    if pd.isna(row['hcnox']):\n",
    "        return median_hcnox_by_fuel_type[row['fuel_type']]\n",
    "    else:\n",
    "        return row['hcnox']\n",
    "\n",
    "data['hcnox'] = data.apply(fill_hcnox, axis=1)\n",
    "\n",
    "# delete rows where urban_cons is nan\n",
    "data.dropna(subset=['urb_cons', 'exturb_cons'], inplace=True)\n",
    "\n",
    "# replace nan values of hcnox by 0 if data['fuel_type'] contains the string 'EH' or 'ES' \n",
    "data.loc[data['fuel_type'].str.contains('EH|ES|GN'), 'hcnox'] = data.loc[data['fuel_type'].str.contains('EH|ES'), 'hcnox'].fillna(0)\n",
    "\n",
    "# same thing with nox and co\n",
    "data.loc[data['fuel_type'].str.contains('EH|ES'), 'nox'] = data.loc[data['fuel_type'].str.contains('EH|ES'), 'nox'].fillna(0)\n",
    "data.loc[data['fuel_type'].str.contains('EH|ES'), 'co'] = data.loc[data['fuel_type'].str.contains('EH|ES'), 'co'].fillna(0)\n",
    "\n",
    "# delete all rows that have nan values for co or nox but not containing 'EH' or 'ES' in fuel_type\n",
    "data.dropna(subset=['nox', 'co'], inplace=True)\n",
    "\n",
    "# replace all nan values in the dataframe by 0\n",
    "data.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On encode ensuite les colonnes qui ont des valeurs non numériques. La librairie sklearn permet de faire cela facilement. On encode les colonnes qui ont des valeurs non numériques en utilisant la méthode `LabelEncoder` de la librairie `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding categorical data\n",
    "labelencoder = {}\n",
    "\n",
    "categorical_cols = ['brand', 'model','car_class','range','fuel_type','hybrid','max_power','grbx_type_ratios']\n",
    "for col in categorical_cols:\n",
    "    labelencoder[col] = LabelEncoder()\n",
    "    data[col] = labelencoder[col].fit_transform(data[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La prochaine étape ensuite consistera en le choix des features qui sont intéressantes à considérer dans notre modèle.\n",
    "Dans un premier temps, on prend toutes les features qui sont numériques. On utilise la méthode `SelectKBest` de la librairie `sklearn` pour sélectionner les features qui sont les plus importantes. On utilise ensuite la méthode `fit_transform` pour transformer les données en utilisant les features sélectionnées.\n",
    "\n",
    "Les émissions de co2 étant l'objectif de notre étude, on isole la colonne afin selectionner les features pertinentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(columns=['co2'])\n",
    "y = data['co2']\n",
    "\n",
    "selector = SelectKBest(score_func= f_classif, k = 16)\n",
    "\n",
    "X_selected = selector.fit_transform(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuite on sépare les données en données d'entraînement et données de test. On utilise la méthode `train_test_split` de la librairie `sklearn` pour cela. On met le paramètre random_state à 0 pour s'assurer d'avoir des résultats reproductibles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_learn, X_test, y_learn, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_learn = scaler.fit_transform(X_learn)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Etant donné l'utilisation de pytorch, il faut convertir les données en tenseurs. On utilise la méthode `torch.tensor` pour cela."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = torch.tensor(X_learn, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_learn.values, dtype=torch.float32).view(-1, 1)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).view(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il est temps de créer notre modèle de regression :D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CO2PredictionModel(nn.Module):\n",
    "    def __init__(self, input_dim) -> None:\n",
    "        super(CO2PredictionModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 64)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.bn2 = nn.BatchNorm1d(32)\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La dernière étape consiste à entrainer le modèle sur les données de training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nour\\OneDrive - INSA Lyon\\Documents\\Cours\\4A\\S2\\Machine Learning\\.venv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Train Loss: 41873.234375 - Validation Loss: 41792.7578125\n",
      "Epoch 10 - Train Loss: 41798.2265625 - Validation Loss: 41715.58984375\n",
      "Epoch 20 - Train Loss: 41680.578125 - Validation Loss: 41591.80859375\n",
      "Epoch 30 - Train Loss: 41485.6796875 - Validation Loss: 41387.6953125\n",
      "Epoch 40 - Train Loss: 41176.83984375 - Validation Loss: 41065.6171875\n",
      "Epoch 50 - Train Loss: 40708.5703125 - Validation Loss: 40580.44921875\n",
      "Epoch 60 - Train Loss: 40033.0078125 - Validation Loss: 39883.515625\n",
      "Epoch 70 - Train Loss: 39090.4765625 - Validation Loss: 38915.44921875\n",
      "Epoch 80 - Train Loss: 37826.6328125 - Validation Loss: 37623.19921875\n",
      "Epoch 90 - Train Loss: 36195.8046875 - Validation Loss: 35962.6015625\n",
      "Epoch 100 - Train Loss: 34165.4375 - Validation Loss: 33903.22265625\n",
      "Epoch 110 - Train Loss: 31721.939453125 - Validation Loss: 31434.90625\n",
      "Epoch 120 - Train Loss: 28893.900390625 - Validation Loss: 28590.451171875\n",
      "Epoch 130 - Train Loss: 25750.658203125 - Validation Loss: 25443.103515625\n",
      "Epoch 140 - Train Loss: 22402.056640625 - Validation Loss: 22105.166015625\n",
      "Epoch 150 - Train Loss: 18991.7578125 - Validation Loss: 18720.93359375\n",
      "Epoch 160 - Train Loss: 15679.2236328125 - Validation Loss: 15447.935546875\n",
      "Epoch 170 - Train Loss: 12616.837890625 - Validation Loss: 12434.2568359375\n",
      "Epoch 180 - Train Loss: 9925.236328125 - Validation Loss: 9794.392578125\n",
      "Epoch 190 - Train Loss: 7673.60205078125 - Validation Loss: 7591.236328125\n",
      "Epoch 200 - Train Loss: 5876.1669921875 - Validation Loss: 5834.37939453125\n",
      "Epoch 210 - Train Loss: 4500.13427734375 - Validation Loss: 4489.62158203125\n",
      "Epoch 220 - Train Loss: 3485.929931640625 - Validation Loss: 3497.952880859375\n",
      "Epoch 230 - Train Loss: 2762.400634765625 - Validation Loss: 2789.446044921875\n",
      "Epoch 240 - Train Loss: 2257.303466796875 - Validation Loss: 2293.505859375\n",
      "Epoch 250 - Train Loss: 1906.9014892578125 - Validation Loss: 1947.6939697265625\n",
      "Epoch 260 - Train Loss: 1658.77490234375 - Validation Loss: 1701.066162109375\n",
      "Epoch 270 - Train Loss: 1475.3648681640625 - Validation Loss: 1517.5885009765625\n",
      "Epoch 280 - Train Loss: 1333.113037109375 - Validation Loss: 1374.3818359375\n",
      "Epoch 290 - Train Loss: 1217.8626708984375 - Validation Loss: 1257.822998046875\n",
      "Epoch 300 - Train Loss: 1121.5037841796875 - Validation Loss: 1159.96875\n",
      "Epoch 310 - Train Loss: 1039.453125 - Validation Loss: 1076.3153076171875\n",
      "Epoch 320 - Train Loss: 968.8895874023438 - Validation Loss: 1004.0709228515625\n",
      "Epoch 330 - Train Loss: 907.5552368164062 - Validation Loss: 940.9697265625\n",
      "Epoch 340 - Train Loss: 853.8162231445312 - Validation Loss: 885.4547729492188\n",
      "Epoch 350 - Train Loss: 806.4473266601562 - Validation Loss: 836.3870849609375\n",
      "Epoch 360 - Train Loss: 764.58447265625 - Validation Loss: 792.9180297851562\n",
      "Epoch 370 - Train Loss: 727.4549560546875 - Validation Loss: 754.2448120117188\n",
      "Epoch 380 - Train Loss: 694.3936157226562 - Validation Loss: 719.694091796875\n",
      "Epoch 390 - Train Loss: 664.7800903320312 - Validation Loss: 688.6480102539062\n",
      "Epoch 400 - Train Loss: 638.1486206054688 - Validation Loss: 660.6171264648438\n",
      "Epoch 410 - Train Loss: 614.0914306640625 - Validation Loss: 635.1849365234375\n",
      "Epoch 420 - Train Loss: 592.2559204101562 - Validation Loss: 612.0608520507812\n",
      "Epoch 430 - Train Loss: 572.3837280273438 - Validation Loss: 590.994873046875\n",
      "Epoch 440 - Train Loss: 554.19921875 - Validation Loss: 571.6995849609375\n",
      "Epoch 450 - Train Loss: 537.4550170898438 - Validation Loss: 553.8871459960938\n",
      "Epoch 460 - Train Loss: 521.927734375 - Validation Loss: 537.3775634765625\n",
      "Epoch 470 - Train Loss: 507.44189453125 - Validation Loss: 521.9768676757812\n",
      "Epoch 480 - Train Loss: 493.8507385253906 - Validation Loss: 507.5260925292969\n",
      "Epoch 490 - Train Loss: 481.0591735839844 - Validation Loss: 493.95574951171875\n",
      "Epoch 500 - Train Loss: 468.978515625 - Validation Loss: 481.1608581542969\n",
      "Epoch 510 - Train Loss: 457.5010070800781 - Validation Loss: 469.0475158691406\n",
      "Epoch 520 - Train Loss: 446.5896911621094 - Validation Loss: 457.5587463378906\n",
      "Epoch 530 - Train Loss: 436.1742858886719 - Validation Loss: 446.6227722167969\n",
      "Epoch 540 - Train Loss: 426.2034912109375 - Validation Loss: 436.1951904296875\n",
      "Epoch 550 - Train Loss: 416.6429443359375 - Validation Loss: 426.23931884765625\n",
      "Epoch 560 - Train Loss: 407.4432373046875 - Validation Loss: 416.69390869140625\n",
      "Epoch 570 - Train Loss: 398.3127746582031 - Validation Loss: 407.1658935546875\n",
      "Epoch 580 - Train Loss: 388.9956970214844 - Validation Loss: 397.7161865234375\n",
      "Epoch 590 - Train Loss: 380.019775390625 - Validation Loss: 388.5905456542969\n",
      "Epoch 600 - Train Loss: 371.36431884765625 - Validation Loss: 379.7664794921875\n",
      "Epoch 610 - Train Loss: 362.9862365722656 - Validation Loss: 371.30078125\n",
      "Epoch 620 - Train Loss: 354.8789367675781 - Validation Loss: 363.1413879394531\n",
      "Epoch 630 - Train Loss: 347.00982666015625 - Validation Loss: 355.2459411621094\n",
      "Epoch 640 - Train Loss: 339.36572265625 - Validation Loss: 347.5992126464844\n",
      "Epoch 650 - Train Loss: 331.9442138671875 - Validation Loss: 340.1852722167969\n",
      "Epoch 660 - Train Loss: 324.7433776855469 - Validation Loss: 332.9807434082031\n",
      "Epoch 670 - Train Loss: 317.7406005859375 - Validation Loss: 325.9815368652344\n",
      "Epoch 680 - Train Loss: 310.9244079589844 - Validation Loss: 319.1786193847656\n",
      "Epoch 690 - Train Loss: 304.2929382324219 - Validation Loss: 312.5628967285156\n",
      "Epoch 700 - Train Loss: 297.8431701660156 - Validation Loss: 306.136474609375\n",
      "Epoch 710 - Train Loss: 291.603271484375 - Validation Loss: 299.8966064453125\n",
      "Epoch 720 - Train Loss: 285.5530700683594 - Validation Loss: 293.8518371582031\n",
      "Epoch 730 - Train Loss: 279.667236328125 - Validation Loss: 287.9768981933594\n",
      "Epoch 740 - Train Loss: 273.9287109375 - Validation Loss: 282.24920654296875\n",
      "Epoch 750 - Train Loss: 268.32012939453125 - Validation Loss: 276.65631103515625\n",
      "Epoch 760 - Train Loss: 262.843994140625 - Validation Loss: 271.2058410644531\n",
      "Epoch 770 - Train Loss: 257.4844970703125 - Validation Loss: 265.87811279296875\n",
      "Epoch 780 - Train Loss: 252.23995971679688 - Validation Loss: 260.6737976074219\n",
      "Epoch 790 - Train Loss: 247.10235595703125 - Validation Loss: 255.5672607421875\n",
      "Epoch 800 - Train Loss: 242.07553100585938 - Validation Loss: 250.57061767578125\n",
      "Epoch 810 - Train Loss: 237.1444091796875 - Validation Loss: 245.66831970214844\n",
      "Epoch 820 - Train Loss: 232.29627990722656 - Validation Loss: 240.8543701171875\n",
      "Epoch 830 - Train Loss: 227.48147583007812 - Validation Loss: 236.0695343017578\n",
      "Epoch 840 - Train Loss: 222.456298828125 - Validation Loss: 231.02780151367188\n",
      "Epoch 850 - Train Loss: 217.48800659179688 - Validation Loss: 226.1268310546875\n",
      "Epoch 860 - Train Loss: 212.6180877685547 - Validation Loss: 221.39315795898438\n",
      "Epoch 870 - Train Loss: 207.8305206298828 - Validation Loss: 216.68270874023438\n",
      "Epoch 880 - Train Loss: 203.19517517089844 - Validation Loss: 212.080810546875\n",
      "Epoch 890 - Train Loss: 198.64244079589844 - Validation Loss: 207.54470825195312\n",
      "Epoch 900 - Train Loss: 194.1484375 - Validation Loss: 203.0242156982422\n",
      "Epoch 910 - Train Loss: 189.75025939941406 - Validation Loss: 198.65509033203125\n",
      "Epoch 920 - Train Loss: 185.4564971923828 - Validation Loss: 194.39210510253906\n",
      "Epoch 930 - Train Loss: 181.2582244873047 - Validation Loss: 190.18260192871094\n",
      "Epoch 940 - Train Loss: 177.16488647460938 - Validation Loss: 186.0931854248047\n",
      "Epoch 950 - Train Loss: 173.18600463867188 - Validation Loss: 182.1305694580078\n",
      "Epoch 960 - Train Loss: 169.31100463867188 - Validation Loss: 178.30218505859375\n",
      "Epoch 970 - Train Loss: 165.49072265625 - Validation Loss: 174.51016235351562\n",
      "Epoch 980 - Train Loss: 161.67111206054688 - Validation Loss: 170.72406005859375\n",
      "Epoch 990 - Train Loss: 157.91921997070312 - Validation Loss: 166.98837280273438\n",
      "Epoch 1000 - Train Loss: 154.33343505859375 - Validation Loss: 163.44100952148438\n",
      "Epoch 1010 - Train Loss: 150.85690307617188 - Validation Loss: 160.00900268554688\n",
      "Epoch 1020 - Train Loss: 147.47679138183594 - Validation Loss: 156.61923217773438\n",
      "Epoch 1030 - Train Loss: 144.18446350097656 - Validation Loss: 153.31924438476562\n",
      "Epoch 1040 - Train Loss: 140.99293518066406 - Validation Loss: 150.13137817382812\n",
      "Epoch 1050 - Train Loss: 137.90887451171875 - Validation Loss: 147.0366973876953\n",
      "Epoch 1060 - Train Loss: 134.9277801513672 - Validation Loss: 144.0411376953125\n",
      "Epoch 1070 - Train Loss: 132.052490234375 - Validation Loss: 141.12905883789062\n",
      "Epoch 1080 - Train Loss: 129.27589416503906 - Validation Loss: 138.3195343017578\n",
      "Epoch 1090 - Train Loss: 126.5860595703125 - Validation Loss: 135.5944061279297\n",
      "Epoch 1100 - Train Loss: 123.97288513183594 - Validation Loss: 132.94186401367188\n",
      "Epoch 1110 - Train Loss: 121.4344711303711 - Validation Loss: 130.3638153076172\n",
      "Epoch 1120 - Train Loss: 118.97371673583984 - Validation Loss: 127.86040496826172\n",
      "Epoch 1130 - Train Loss: 116.58629608154297 - Validation Loss: 125.42338562011719\n",
      "Epoch 1140 - Train Loss: 114.27523040771484 - Validation Loss: 123.04325103759766\n",
      "Epoch 1150 - Train Loss: 112.03699493408203 - Validation Loss: 120.74008178710938\n",
      "Epoch 1160 - Train Loss: 109.86852264404297 - Validation Loss: 118.51539611816406\n",
      "Epoch 1170 - Train Loss: 107.761474609375 - Validation Loss: 116.35893249511719\n",
      "Epoch 1180 - Train Loss: 105.71839904785156 - Validation Loss: 114.26649475097656\n",
      "Epoch 1190 - Train Loss: 103.73638916015625 - Validation Loss: 112.23095703125\n",
      "Epoch 1200 - Train Loss: 101.81539154052734 - Validation Loss: 110.24677276611328\n",
      "Epoch 1210 - Train Loss: 99.95962524414062 - Validation Loss: 108.31033325195312\n",
      "Epoch 1220 - Train Loss: 98.16471099853516 - Validation Loss: 106.441650390625\n",
      "Epoch 1230 - Train Loss: 96.42755126953125 - Validation Loss: 104.62876892089844\n",
      "Epoch 1240 - Train Loss: 94.74112701416016 - Validation Loss: 102.85462951660156\n",
      "Epoch 1250 - Train Loss: 93.10459899902344 - Validation Loss: 101.13605499267578\n",
      "Epoch 1260 - Train Loss: 91.51991271972656 - Validation Loss: 99.47048950195312\n",
      "Epoch 1270 - Train Loss: 89.98314666748047 - Validation Loss: 97.85759735107422\n",
      "Epoch 1280 - Train Loss: 88.49060821533203 - Validation Loss: 96.2989730834961\n",
      "Epoch 1290 - Train Loss: 87.04387664794922 - Validation Loss: 94.78451538085938\n",
      "Epoch 1300 - Train Loss: 85.6451187133789 - Validation Loss: 93.30667877197266\n",
      "Epoch 1310 - Train Loss: 84.2921371459961 - Validation Loss: 91.86875915527344\n",
      "Epoch 1320 - Train Loss: 82.9874267578125 - Validation Loss: 90.49457550048828\n",
      "Epoch 1330 - Train Loss: 81.72666931152344 - Validation Loss: 89.16378021240234\n",
      "Epoch 1340 - Train Loss: 80.5009536743164 - Validation Loss: 87.86876678466797\n",
      "Epoch 1350 - Train Loss: 79.3123550415039 - Validation Loss: 86.61302947998047\n",
      "Epoch 1360 - Train Loss: 78.15923309326172 - Validation Loss: 85.39259338378906\n",
      "Epoch 1370 - Train Loss: 77.03825378417969 - Validation Loss: 84.19026184082031\n",
      "Epoch 1380 - Train Loss: 75.95095825195312 - Validation Loss: 83.02362823486328\n",
      "Epoch 1390 - Train Loss: 74.88772583007812 - Validation Loss: 81.88909149169922\n",
      "Epoch 1400 - Train Loss: 73.84738159179688 - Validation Loss: 80.77728271484375\n",
      "Epoch 1410 - Train Loss: 72.83257293701172 - Validation Loss: 79.70089721679688\n",
      "Epoch 1420 - Train Loss: 71.83940887451172 - Validation Loss: 78.63721466064453\n",
      "Epoch 1430 - Train Loss: 70.87046813964844 - Validation Loss: 77.59310150146484\n",
      "Epoch 1440 - Train Loss: 69.92593383789062 - Validation Loss: 76.58079528808594\n",
      "Epoch 1450 - Train Loss: 69.00328826904297 - Validation Loss: 75.59432983398438\n",
      "Epoch 1460 - Train Loss: 68.10415649414062 - Validation Loss: 74.62922668457031\n",
      "Epoch 1470 - Train Loss: 67.23055267333984 - Validation Loss: 73.69703674316406\n",
      "Epoch 1480 - Train Loss: 66.38088989257812 - Validation Loss: 72.78449249267578\n",
      "Epoch 1490 - Train Loss: 65.54527282714844 - Validation Loss: 71.89250183105469\n",
      "Epoch 1500 - Train Loss: 64.72806549072266 - Validation Loss: 71.02542114257812\n",
      "Epoch 1510 - Train Loss: 63.929298400878906 - Validation Loss: 70.17951202392578\n",
      "Epoch 1520 - Train Loss: 63.148406982421875 - Validation Loss: 69.34882354736328\n",
      "Epoch 1530 - Train Loss: 62.383827209472656 - Validation Loss: 68.5342025756836\n",
      "Epoch 1540 - Train Loss: 61.63550567626953 - Validation Loss: 67.74567413330078\n",
      "Epoch 1550 - Train Loss: 60.90481948852539 - Validation Loss: 66.96896362304688\n",
      "Epoch 1560 - Train Loss: 60.191253662109375 - Validation Loss: 66.20396423339844\n",
      "Epoch 1570 - Train Loss: 59.49216842651367 - Validation Loss: 65.45356750488281\n",
      "Epoch 1580 - Train Loss: 58.806190490722656 - Validation Loss: 64.71675109863281\n",
      "Epoch 1590 - Train Loss: 58.13447189331055 - Validation Loss: 63.99075698852539\n",
      "Epoch 1600 - Train Loss: 57.47625732421875 - Validation Loss: 63.27937698364258\n",
      "Epoch 1610 - Train Loss: 56.828243255615234 - Validation Loss: 62.586856842041016\n",
      "Epoch 1620 - Train Loss: 56.18470764160156 - Validation Loss: 61.90563201904297\n",
      "Epoch 1630 - Train Loss: 55.540164947509766 - Validation Loss: 61.22842025756836\n",
      "Epoch 1640 - Train Loss: 54.9062385559082 - Validation Loss: 60.552555084228516\n",
      "Epoch 1650 - Train Loss: 54.28325653076172 - Validation Loss: 59.90810775756836\n",
      "Epoch 1660 - Train Loss: 53.66807556152344 - Validation Loss: 59.25999450683594\n",
      "Epoch 1670 - Train Loss: 53.060420989990234 - Validation Loss: 58.6152229309082\n",
      "Epoch 1680 - Train Loss: 52.461334228515625 - Validation Loss: 57.97421646118164\n",
      "Epoch 1690 - Train Loss: 51.87001419067383 - Validation Loss: 57.33585739135742\n",
      "Epoch 1700 - Train Loss: 51.286800384521484 - Validation Loss: 56.71082305908203\n",
      "Epoch 1710 - Train Loss: 50.71248245239258 - Validation Loss: 56.09774398803711\n",
      "Epoch 1720 - Train Loss: 50.14802169799805 - Validation Loss: 55.50103759765625\n",
      "Epoch 1730 - Train Loss: 49.59130859375 - Validation Loss: 54.90304946899414\n",
      "Epoch 1740 - Train Loss: 49.0421257019043 - Validation Loss: 54.30903244018555\n",
      "Epoch 1750 - Train Loss: 48.50150680541992 - Validation Loss: 53.724334716796875\n",
      "Epoch 1760 - Train Loss: 47.96698760986328 - Validation Loss: 53.14802932739258\n",
      "Epoch 1770 - Train Loss: 47.43852615356445 - Validation Loss: 52.56867599487305\n",
      "Epoch 1780 - Train Loss: 46.915245056152344 - Validation Loss: 51.995330810546875\n",
      "Epoch 1790 - Train Loss: 46.39744567871094 - Validation Loss: 51.425628662109375\n",
      "Epoch 1800 - Train Loss: 45.882225036621094 - Validation Loss: 50.86549758911133\n",
      "Epoch 1810 - Train Loss: 45.36719512939453 - Validation Loss: 50.307403564453125\n",
      "Epoch 1820 - Train Loss: 44.85139083862305 - Validation Loss: 49.741416931152344\n",
      "Epoch 1830 - Train Loss: 44.325679779052734 - Validation Loss: 49.16938781738281\n",
      "Epoch 1840 - Train Loss: 43.805728912353516 - Validation Loss: 48.598052978515625\n",
      "Epoch 1850 - Train Loss: 43.28544998168945 - Validation Loss: 48.04648971557617\n",
      "Epoch 1860 - Train Loss: 42.773193359375 - Validation Loss: 47.49611282348633\n",
      "Epoch 1870 - Train Loss: 42.2717399597168 - Validation Loss: 46.96376419067383\n",
      "Epoch 1880 - Train Loss: 41.78341293334961 - Validation Loss: 46.42607498168945\n",
      "Epoch 1890 - Train Loss: 41.30638122558594 - Validation Loss: 45.899539947509766\n",
      "Epoch 1900 - Train Loss: 40.83700180053711 - Validation Loss: 45.374671936035156\n",
      "Epoch 1910 - Train Loss: 40.3726921081543 - Validation Loss: 44.843910217285156\n",
      "Epoch 1920 - Train Loss: 39.913665771484375 - Validation Loss: 44.32380676269531\n",
      "Epoch 1930 - Train Loss: 39.45648956298828 - Validation Loss: 43.806663513183594\n",
      "Epoch 1940 - Train Loss: 39.002323150634766 - Validation Loss: 43.29230499267578\n",
      "Epoch 1950 - Train Loss: 38.52964401245117 - Validation Loss: 42.7481803894043\n",
      "Epoch 1960 - Train Loss: 38.060428619384766 - Validation Loss: 42.21691131591797\n",
      "Epoch 1970 - Train Loss: 37.60551071166992 - Validation Loss: 41.71364974975586\n",
      "Epoch 1980 - Train Loss: 37.16091537475586 - Validation Loss: 41.213218688964844\n",
      "Epoch 1990 - Train Loss: 36.723915100097656 - Validation Loss: 40.709774017333984\n",
      "Epoch 2000 - Train Loss: 36.29655838012695 - Validation Loss: 40.21047592163086\n",
      "Epoch 2010 - Train Loss: 35.87311935424805 - Validation Loss: 39.70878982543945\n",
      "Epoch 2020 - Train Loss: 35.4600715637207 - Validation Loss: 39.2303352355957\n",
      "Epoch 2030 - Train Loss: 35.060916900634766 - Validation Loss: 38.77909851074219\n",
      "Epoch 2040 - Train Loss: 34.674198150634766 - Validation Loss: 38.331626892089844\n",
      "Epoch 2050 - Train Loss: 34.29800796508789 - Validation Loss: 37.899696350097656\n",
      "Epoch 2060 - Train Loss: 33.92877960205078 - Validation Loss: 37.47666549682617\n",
      "Epoch 2070 - Train Loss: 33.567115783691406 - Validation Loss: 37.065345764160156\n",
      "Epoch 2080 - Train Loss: 33.21364974975586 - Validation Loss: 36.659603118896484\n",
      "Epoch 2090 - Train Loss: 32.8674201965332 - Validation Loss: 36.26609802246094\n",
      "Epoch 2100 - Train Loss: 32.52553939819336 - Validation Loss: 35.87548065185547\n",
      "Epoch 2110 - Train Loss: 32.18857192993164 - Validation Loss: 35.48872375488281\n",
      "Epoch 2120 - Train Loss: 31.85317611694336 - Validation Loss: 35.09743118286133\n",
      "Epoch 2130 - Train Loss: 31.521141052246094 - Validation Loss: 34.717864990234375\n",
      "Epoch 2140 - Train Loss: 31.194374084472656 - Validation Loss: 34.34471893310547\n",
      "Epoch 2150 - Train Loss: 30.871835708618164 - Validation Loss: 33.97699737548828\n",
      "Epoch 2160 - Train Loss: 30.551542282104492 - Validation Loss: 33.5958366394043\n",
      "Epoch 2170 - Train Loss: 30.234943389892578 - Validation Loss: 33.223899841308594\n",
      "Epoch 2180 - Train Loss: 29.923254013061523 - Validation Loss: 32.862281799316406\n",
      "Epoch 2190 - Train Loss: 29.617441177368164 - Validation Loss: 32.51231002807617\n",
      "Epoch 2200 - Train Loss: 29.316713333129883 - Validation Loss: 32.16856384277344\n",
      "Epoch 2210 - Train Loss: 29.0195369720459 - Validation Loss: 31.830867767333984\n",
      "Epoch 2220 - Train Loss: 28.72585678100586 - Validation Loss: 31.494657516479492\n",
      "Epoch 2230 - Train Loss: 28.43683624267578 - Validation Loss: 31.163047790527344\n",
      "Epoch 2240 - Train Loss: 28.151227951049805 - Validation Loss: 30.833667755126953\n",
      "Epoch 2250 - Train Loss: 27.86966896057129 - Validation Loss: 30.50995635986328\n",
      "Epoch 2260 - Train Loss: 27.594070434570312 - Validation Loss: 30.1973819732666\n",
      "Epoch 2270 - Train Loss: 27.324077606201172 - Validation Loss: 29.895296096801758\n",
      "Epoch 2280 - Train Loss: 27.057941436767578 - Validation Loss: 29.60126495361328\n",
      "Epoch 2290 - Train Loss: 26.795578002929688 - Validation Loss: 29.311052322387695\n",
      "Epoch 2300 - Train Loss: 26.536344528198242 - Validation Loss: 29.016830444335938\n",
      "Epoch 2310 - Train Loss: 26.281953811645508 - Validation Loss: 28.733291625976562\n",
      "Epoch 2320 - Train Loss: 26.03247833251953 - Validation Loss: 28.453170776367188\n",
      "Epoch 2330 - Train Loss: 25.787981033325195 - Validation Loss: 28.177997589111328\n",
      "Epoch 2340 - Train Loss: 25.547420501708984 - Validation Loss: 27.914527893066406\n",
      "Epoch 2350 - Train Loss: 25.31157112121582 - Validation Loss: 27.655162811279297\n",
      "Epoch 2360 - Train Loss: 25.080244064331055 - Validation Loss: 27.404842376708984\n",
      "Epoch 2370 - Train Loss: 24.853410720825195 - Validation Loss: 27.160751342773438\n",
      "Epoch 2380 - Train Loss: 24.63081932067871 - Validation Loss: 26.922292709350586\n",
      "Epoch 2390 - Train Loss: 24.413118362426758 - Validation Loss: 26.691329956054688\n",
      "Epoch 2400 - Train Loss: 24.199806213378906 - Validation Loss: 26.46476936340332\n",
      "Epoch 2410 - Train Loss: 23.98981475830078 - Validation Loss: 26.244115829467773\n",
      "Epoch 2420 - Train Loss: 23.782623291015625 - Validation Loss: 26.024019241333008\n",
      "Epoch 2430 - Train Loss: 23.578384399414062 - Validation Loss: 25.803556442260742\n",
      "Epoch 2440 - Train Loss: 23.377811431884766 - Validation Loss: 25.59120750427246\n",
      "Epoch 2450 - Train Loss: 23.1816349029541 - Validation Loss: 25.38233184814453\n",
      "Epoch 2460 - Train Loss: 22.98732566833496 - Validation Loss: 25.175334930419922\n",
      "Epoch 2470 - Train Loss: 22.79575538635254 - Validation Loss: 24.96906852722168\n",
      "Epoch 2480 - Train Loss: 22.6073055267334 - Validation Loss: 24.767189025878906\n",
      "Epoch 2490 - Train Loss: 22.421953201293945 - Validation Loss: 24.571081161499023\n",
      "Epoch 2500 - Train Loss: 22.239425659179688 - Validation Loss: 24.37588882446289\n",
      "Epoch 2510 - Train Loss: 22.059314727783203 - Validation Loss: 24.18776512145996\n",
      "Epoch 2520 - Train Loss: 21.881790161132812 - Validation Loss: 23.999462127685547\n",
      "Epoch 2530 - Train Loss: 21.70701026916504 - Validation Loss: 23.81371307373047\n",
      "Epoch 2540 - Train Loss: 21.533851623535156 - Validation Loss: 23.631061553955078\n",
      "Epoch 2550 - Train Loss: 21.36298942565918 - Validation Loss: 23.451780319213867\n",
      "Epoch 2560 - Train Loss: 21.19407081604004 - Validation Loss: 23.276226043701172\n",
      "Epoch 2570 - Train Loss: 21.0268497467041 - Validation Loss: 23.099149703979492\n",
      "Epoch 2580 - Train Loss: 20.86202049255371 - Validation Loss: 22.925275802612305\n",
      "Epoch 2590 - Train Loss: 20.699878692626953 - Validation Loss: 22.748449325561523\n",
      "Epoch 2600 - Train Loss: 20.540565490722656 - Validation Loss: 22.575929641723633\n",
      "Epoch 2610 - Train Loss: 20.38412094116211 - Validation Loss: 22.408212661743164\n",
      "Epoch 2620 - Train Loss: 20.230323791503906 - Validation Loss: 22.24266242980957\n",
      "Epoch 2630 - Train Loss: 20.078937530517578 - Validation Loss: 22.081451416015625\n",
      "Epoch 2640 - Train Loss: 19.929977416992188 - Validation Loss: 21.919580459594727\n",
      "Epoch 2650 - Train Loss: 19.783851623535156 - Validation Loss: 21.759920120239258\n",
      "Epoch 2660 - Train Loss: 19.640743255615234 - Validation Loss: 21.609342575073242\n",
      "Epoch 2670 - Train Loss: 19.498821258544922 - Validation Loss: 21.46013069152832\n",
      "Epoch 2680 - Train Loss: 19.35806655883789 - Validation Loss: 21.308435440063477\n",
      "Epoch 2690 - Train Loss: 19.219120025634766 - Validation Loss: 21.15674591064453\n",
      "Epoch 2700 - Train Loss: 19.081199645996094 - Validation Loss: 21.004337310791016\n",
      "Epoch 2710 - Train Loss: 18.944713592529297 - Validation Loss: 20.855430603027344\n",
      "Epoch 2720 - Train Loss: 18.810426712036133 - Validation Loss: 20.71123695373535\n",
      "Epoch 2730 - Train Loss: 18.67875099182129 - Validation Loss: 20.568401336669922\n",
      "Epoch 2740 - Train Loss: 18.549283981323242 - Validation Loss: 20.42770767211914\n",
      "Epoch 2750 - Train Loss: 18.4215030670166 - Validation Loss: 20.291221618652344\n",
      "Epoch 2760 - Train Loss: 18.296310424804688 - Validation Loss: 20.15545082092285\n",
      "Epoch 2770 - Train Loss: 18.17325210571289 - Validation Loss: 20.02310562133789\n",
      "Epoch 2780 - Train Loss: 18.051971435546875 - Validation Loss: 19.893301010131836\n",
      "Epoch 2790 - Train Loss: 17.931751251220703 - Validation Loss: 19.764436721801758\n",
      "Epoch 2800 - Train Loss: 17.812580108642578 - Validation Loss: 19.635229110717773\n",
      "Epoch 2810 - Train Loss: 17.6934871673584 - Validation Loss: 19.503671646118164\n",
      "Epoch 2820 - Train Loss: 17.57301902770996 - Validation Loss: 19.369455337524414\n",
      "Epoch 2830 - Train Loss: 17.455036163330078 - Validation Loss: 19.239221572875977\n",
      "Epoch 2840 - Train Loss: 17.34078598022461 - Validation Loss: 19.115137100219727\n",
      "Epoch 2850 - Train Loss: 17.229808807373047 - Validation Loss: 18.994062423706055\n",
      "Epoch 2860 - Train Loss: 17.12143325805664 - Validation Loss: 18.87726593017578\n",
      "Epoch 2870 - Train Loss: 17.01486587524414 - Validation Loss: 18.758543014526367\n",
      "Epoch 2880 - Train Loss: 16.9100341796875 - Validation Loss: 18.643156051635742\n",
      "Epoch 2890 - Train Loss: 16.806297302246094 - Validation Loss: 18.52800941467285\n",
      "Epoch 2900 - Train Loss: 16.702632904052734 - Validation Loss: 18.412343978881836\n",
      "Epoch 2910 - Train Loss: 16.59792709350586 - Validation Loss: 18.294239044189453\n",
      "Epoch 2920 - Train Loss: 16.491731643676758 - Validation Loss: 18.177278518676758\n",
      "Epoch 2930 - Train Loss: 16.38665008544922 - Validation Loss: 18.062938690185547\n",
      "Epoch 2940 - Train Loss: 16.282468795776367 - Validation Loss: 17.952470779418945\n",
      "Epoch 2950 - Train Loss: 16.178569793701172 - Validation Loss: 17.841875076293945\n",
      "Epoch 2960 - Train Loss: 16.076398849487305 - Validation Loss: 17.733579635620117\n",
      "Epoch 2970 - Train Loss: 15.976005554199219 - Validation Loss: 17.632240295410156\n",
      "Epoch 2980 - Train Loss: 15.876913070678711 - Validation Loss: 17.525732040405273\n",
      "Epoch 2990 - Train Loss: 15.779182434082031 - Validation Loss: 17.418729782104492\n",
      "Epoch 3000 - Train Loss: 15.680678367614746 - Validation Loss: 17.31291389465332\n",
      "Epoch 3010 - Train Loss: 15.58288288116455 - Validation Loss: 17.2132625579834\n",
      "Epoch 3020 - Train Loss: 15.485739707946777 - Validation Loss: 17.114728927612305\n",
      "Epoch 3030 - Train Loss: 15.389994621276855 - Validation Loss: 17.016096115112305\n",
      "Epoch 3040 - Train Loss: 15.295159339904785 - Validation Loss: 16.916378021240234\n",
      "Epoch 3050 - Train Loss: 15.201282501220703 - Validation Loss: 16.81745719909668\n",
      "Epoch 3060 - Train Loss: 15.10836410522461 - Validation Loss: 16.72043228149414\n",
      "Epoch 3070 - Train Loss: 15.01667594909668 - Validation Loss: 16.624265670776367\n",
      "Epoch 3080 - Train Loss: 14.925820350646973 - Validation Loss: 16.527345657348633\n",
      "Epoch 3090 - Train Loss: 14.835268020629883 - Validation Loss: 16.432008743286133\n",
      "Epoch 3100 - Train Loss: 14.745659828186035 - Validation Loss: 16.337392807006836\n",
      "Epoch 3110 - Train Loss: 14.656766891479492 - Validation Loss: 16.24209213256836\n",
      "Epoch 3120 - Train Loss: 14.568507194519043 - Validation Loss: 16.14720916748047\n",
      "Epoch 3130 - Train Loss: 14.4804048538208 - Validation Loss: 16.051441192626953\n",
      "Epoch 3140 - Train Loss: 14.391552925109863 - Validation Loss: 15.952958106994629\n",
      "Epoch 3150 - Train Loss: 14.303460121154785 - Validation Loss: 15.855963706970215\n",
      "Epoch 3160 - Train Loss: 14.216241836547852 - Validation Loss: 15.765363693237305\n",
      "Epoch 3170 - Train Loss: 14.13011646270752 - Validation Loss: 15.67214298248291\n",
      "Epoch 3180 - Train Loss: 14.04471206665039 - Validation Loss: 15.576105117797852\n",
      "Epoch 3190 - Train Loss: 13.95908260345459 - Validation Loss: 15.48452091217041\n",
      "Epoch 3200 - Train Loss: 13.873159408569336 - Validation Loss: 15.391006469726562\n",
      "Epoch 3210 - Train Loss: 13.788399696350098 - Validation Loss: 15.296486854553223\n",
      "Epoch 3220 - Train Loss: 13.704039573669434 - Validation Loss: 15.207194328308105\n",
      "Epoch 3230 - Train Loss: 13.619885444641113 - Validation Loss: 15.115774154663086\n",
      "Epoch 3240 - Train Loss: 13.53752326965332 - Validation Loss: 15.027002334594727\n",
      "Epoch 3250 - Train Loss: 13.4562406539917 - Validation Loss: 14.938536643981934\n",
      "Epoch 3260 - Train Loss: 13.37574577331543 - Validation Loss: 14.849287986755371\n",
      "Epoch 3270 - Train Loss: 13.295681953430176 - Validation Loss: 14.759140014648438\n",
      "Epoch 3280 - Train Loss: 13.215648651123047 - Validation Loss: 14.671557426452637\n",
      "Epoch 3290 - Train Loss: 13.135894775390625 - Validation Loss: 14.583287239074707\n",
      "Epoch 3300 - Train Loss: 13.056447982788086 - Validation Loss: 14.492493629455566\n",
      "Epoch 3310 - Train Loss: 12.977251052856445 - Validation Loss: 14.403478622436523\n",
      "Epoch 3320 - Train Loss: 12.898641586303711 - Validation Loss: 14.317399024963379\n",
      "Epoch 3330 - Train Loss: 12.821341514587402 - Validation Loss: 14.232218742370605\n",
      "Epoch 3340 - Train Loss: 12.745505332946777 - Validation Loss: 14.14948844909668\n",
      "Epoch 3350 - Train Loss: 12.670639038085938 - Validation Loss: 14.064199447631836\n",
      "Epoch 3360 - Train Loss: 12.596134185791016 - Validation Loss: 13.985382080078125\n",
      "Epoch 3370 - Train Loss: 12.521591186523438 - Validation Loss: 13.906044006347656\n",
      "Epoch 3380 - Train Loss: 12.4461669921875 - Validation Loss: 13.820924758911133\n",
      "Epoch 3390 - Train Loss: 12.371230125427246 - Validation Loss: 13.739481925964355\n",
      "Epoch 3400 - Train Loss: 12.29556655883789 - Validation Loss: 13.656328201293945\n",
      "Epoch 3410 - Train Loss: 12.220050811767578 - Validation Loss: 13.574362754821777\n",
      "Epoch 3420 - Train Loss: 12.14563274383545 - Validation Loss: 13.494210243225098\n",
      "Epoch 3430 - Train Loss: 12.070913314819336 - Validation Loss: 13.415993690490723\n",
      "Epoch 3440 - Train Loss: 11.996761322021484 - Validation Loss: 13.336881637573242\n",
      "Epoch 3450 - Train Loss: 11.922698974609375 - Validation Loss: 13.257917404174805\n",
      "Epoch 3460 - Train Loss: 11.8486967086792 - Validation Loss: 13.179337501525879\n",
      "Epoch 3470 - Train Loss: 11.774758338928223 - Validation Loss: 13.098299026489258\n",
      "Epoch 3480 - Train Loss: 11.702876091003418 - Validation Loss: 13.02351188659668\n",
      "Epoch 3490 - Train Loss: 11.631742477416992 - Validation Loss: 12.948905944824219\n",
      "Epoch 3500 - Train Loss: 11.56175708770752 - Validation Loss: 12.876605033874512\n",
      "Epoch 3510 - Train Loss: 11.492833137512207 - Validation Loss: 12.803287506103516\n",
      "Epoch 3520 - Train Loss: 11.424781799316406 - Validation Loss: 12.730515480041504\n",
      "Epoch 3530 - Train Loss: 11.356941223144531 - Validation Loss: 12.658541679382324\n",
      "Epoch 3540 - Train Loss: 11.290262222290039 - Validation Loss: 12.58764934539795\n",
      "Epoch 3550 - Train Loss: 11.224869728088379 - Validation Loss: 12.51761245727539\n",
      "Epoch 3560 - Train Loss: 11.160528182983398 - Validation Loss: 12.447469711303711\n",
      "Epoch 3570 - Train Loss: 11.097046852111816 - Validation Loss: 12.382979393005371\n",
      "Epoch 3580 - Train Loss: 11.034862518310547 - Validation Loss: 12.313647270202637\n",
      "Epoch 3590 - Train Loss: 10.9734525680542 - Validation Loss: 12.245701789855957\n",
      "Epoch 3600 - Train Loss: 10.912436485290527 - Validation Loss: 12.178372383117676\n",
      "Epoch 3610 - Train Loss: 10.852028846740723 - Validation Loss: 12.111417770385742\n",
      "Epoch 3620 - Train Loss: 10.791977882385254 - Validation Loss: 12.046353340148926\n",
      "Epoch 3630 - Train Loss: 10.732282638549805 - Validation Loss: 11.980154037475586\n",
      "Epoch 3640 - Train Loss: 10.673175811767578 - Validation Loss: 11.9147310256958\n",
      "Epoch 3650 - Train Loss: 10.614523887634277 - Validation Loss: 11.848892211914062\n",
      "Epoch 3660 - Train Loss: 10.556132316589355 - Validation Loss: 11.783052444458008\n",
      "Epoch 3670 - Train Loss: 10.497946739196777 - Validation Loss: 11.718830108642578\n",
      "Epoch 3680 - Train Loss: 10.440381050109863 - Validation Loss: 11.65600872039795\n",
      "Epoch 3690 - Train Loss: 10.383367538452148 - Validation Loss: 11.594337463378906\n",
      "Epoch 3700 - Train Loss: 10.326956748962402 - Validation Loss: 11.533180236816406\n",
      "Epoch 3710 - Train Loss: 10.271210670471191 - Validation Loss: 11.472262382507324\n",
      "Epoch 3720 - Train Loss: 10.215461730957031 - Validation Loss: 11.409553527832031\n",
      "Epoch 3730 - Train Loss: 10.1603364944458 - Validation Loss: 11.351131439208984\n",
      "Epoch 3740 - Train Loss: 10.105903625488281 - Validation Loss: 11.29001522064209\n",
      "Epoch 3750 - Train Loss: 10.052326202392578 - Validation Loss: 11.23171329498291\n",
      "Epoch 3760 - Train Loss: 9.999302864074707 - Validation Loss: 11.172513961791992\n",
      "Epoch 3770 - Train Loss: 9.946772575378418 - Validation Loss: 11.11225700378418\n",
      "Epoch 3780 - Train Loss: 9.894767761230469 - Validation Loss: 11.055336952209473\n",
      "Epoch 3790 - Train Loss: 9.84298038482666 - Validation Loss: 10.99663257598877\n",
      "Epoch 3800 - Train Loss: 9.791260719299316 - Validation Loss: 10.936758995056152\n",
      "Epoch 3810 - Train Loss: 9.739975929260254 - Validation Loss: 10.87929630279541\n",
      "Epoch 3820 - Train Loss: 9.688665390014648 - Validation Loss: 10.822833061218262\n",
      "Epoch 3830 - Train Loss: 9.63746166229248 - Validation Loss: 10.765028953552246\n",
      "Epoch 3840 - Train Loss: 9.586308479309082 - Validation Loss: 10.70914363861084\n",
      "Epoch 3850 - Train Loss: 9.535367012023926 - Validation Loss: 10.65076732635498\n",
      "Epoch 3860 - Train Loss: 9.484657287597656 - Validation Loss: 10.595671653747559\n",
      "Epoch 3870 - Train Loss: 9.434565544128418 - Validation Loss: 10.539801597595215\n",
      "Epoch 3880 - Train Loss: 9.383927345275879 - Validation Loss: 10.482867240905762\n",
      "Epoch 3890 - Train Loss: 9.332640647888184 - Validation Loss: 10.427430152893066\n",
      "Epoch 3900 - Train Loss: 9.281123161315918 - Validation Loss: 10.372097969055176\n",
      "Epoch 3910 - Train Loss: 9.229559898376465 - Validation Loss: 10.31334114074707\n",
      "Epoch 3920 - Train Loss: 9.177681922912598 - Validation Loss: 10.258096694946289\n",
      "Epoch 3930 - Train Loss: 9.123886108398438 - Validation Loss: 10.203023910522461\n",
      "Epoch 3940 - Train Loss: 9.070460319519043 - Validation Loss: 10.14450454711914\n",
      "Epoch 3950 - Train Loss: 9.017351150512695 - Validation Loss: 10.08311653137207\n",
      "Epoch 3960 - Train Loss: 8.964517593383789 - Validation Loss: 10.02234935760498\n",
      "Epoch 3970 - Train Loss: 8.912124633789062 - Validation Loss: 9.96387004852295\n",
      "Epoch 3980 - Train Loss: 8.859997749328613 - Validation Loss: 9.905606269836426\n",
      "Epoch 3990 - Train Loss: 8.808146476745605 - Validation Loss: 9.848164558410645\n",
      "Epoch 4000 - Train Loss: 8.756353378295898 - Validation Loss: 9.791015625\n",
      "Epoch 4010 - Train Loss: 8.704789161682129 - Validation Loss: 9.73550796508789\n",
      "Epoch 4020 - Train Loss: 8.653959274291992 - Validation Loss: 9.680068969726562\n",
      "Epoch 4030 - Train Loss: 8.603442192077637 - Validation Loss: 9.62657356262207\n",
      "Epoch 4040 - Train Loss: 8.55288314819336 - Validation Loss: 9.571462631225586\n",
      "Epoch 4050 - Train Loss: 8.502527236938477 - Validation Loss: 9.517546653747559\n",
      "Epoch 4060 - Train Loss: 8.452458381652832 - Validation Loss: 9.463810920715332\n",
      "Epoch 4070 - Train Loss: 8.402610778808594 - Validation Loss: 9.410990715026855\n",
      "Epoch 4080 - Train Loss: 8.353023529052734 - Validation Loss: 9.358007431030273\n",
      "Epoch 4090 - Train Loss: 8.303420066833496 - Validation Loss: 9.303444862365723\n",
      "Epoch 4100 - Train Loss: 8.254118919372559 - Validation Loss: 9.248290061950684\n",
      "Epoch 4110 - Train Loss: 8.20483112335205 - Validation Loss: 9.192137718200684\n",
      "Epoch 4120 - Train Loss: 8.155303001403809 - Validation Loss: 9.1366605758667\n",
      "Epoch 4130 - Train Loss: 8.10538387298584 - Validation Loss: 9.082337379455566\n",
      "Epoch 4140 - Train Loss: 8.0556640625 - Validation Loss: 9.028861999511719\n",
      "Epoch 4150 - Train Loss: 8.006341934204102 - Validation Loss: 8.974834442138672\n",
      "Epoch 4160 - Train Loss: 7.957494735717773 - Validation Loss: 8.920947074890137\n",
      "Epoch 4170 - Train Loss: 7.908374786376953 - Validation Loss: 8.86738395690918\n",
      "Epoch 4180 - Train Loss: 7.859031677246094 - Validation Loss: 8.816208839416504\n",
      "Epoch 4190 - Train Loss: 7.8094916343688965 - Validation Loss: 8.76405143737793\n",
      "Epoch 4200 - Train Loss: 7.760197639465332 - Validation Loss: 8.71210765838623\n",
      "Epoch 4210 - Train Loss: 7.711462497711182 - Validation Loss: 8.659780502319336\n",
      "Epoch 4220 - Train Loss: 7.663132190704346 - Validation Loss: 8.607027053833008\n",
      "Epoch 4230 - Train Loss: 7.615063190460205 - Validation Loss: 8.55489444732666\n",
      "Epoch 4240 - Train Loss: 7.567103862762451 - Validation Loss: 8.501649856567383\n",
      "Epoch 4250 - Train Loss: 7.518365383148193 - Validation Loss: 8.447851181030273\n",
      "Epoch 4260 - Train Loss: 7.469387054443359 - Validation Loss: 8.392558097839355\n",
      "Epoch 4270 - Train Loss: 7.420355796813965 - Validation Loss: 8.336731910705566\n",
      "Epoch 4280 - Train Loss: 7.371263027191162 - Validation Loss: 8.282470703125\n",
      "Epoch 4290 - Train Loss: 7.323129177093506 - Validation Loss: 8.230369567871094\n",
      "Epoch 4300 - Train Loss: 7.276772975921631 - Validation Loss: 8.179859161376953\n",
      "Epoch 4310 - Train Loss: 7.231259346008301 - Validation Loss: 8.13062572479248\n",
      "Epoch 4320 - Train Loss: 7.18598747253418 - Validation Loss: 8.084071159362793\n",
      "Epoch 4330 - Train Loss: 7.140381336212158 - Validation Loss: 8.036911010742188\n",
      "Epoch 4340 - Train Loss: 7.095104217529297 - Validation Loss: 7.989607334136963\n",
      "Epoch 4350 - Train Loss: 7.0506157875061035 - Validation Loss: 7.941001892089844\n",
      "Epoch 4360 - Train Loss: 7.006560802459717 - Validation Loss: 7.891486167907715\n",
      "Epoch 4370 - Train Loss: 6.962883472442627 - Validation Loss: 7.84088134765625\n",
      "Epoch 4380 - Train Loss: 6.919723987579346 - Validation Loss: 7.792213439941406\n",
      "Epoch 4390 - Train Loss: 6.876996040344238 - Validation Loss: 7.743885517120361\n",
      "Epoch 4400 - Train Loss: 6.834639072418213 - Validation Loss: 7.697038650512695\n",
      "Epoch 4410 - Train Loss: 6.791601657867432 - Validation Loss: 7.648754596710205\n",
      "Epoch 4420 - Train Loss: 6.748945236206055 - Validation Loss: 7.600842475891113\n",
      "Epoch 4430 - Train Loss: 6.706367015838623 - Validation Loss: 7.552193641662598\n",
      "Epoch 4440 - Train Loss: 6.6645684242248535 - Validation Loss: 7.504966735839844\n",
      "Epoch 4450 - Train Loss: 6.623199462890625 - Validation Loss: 7.458645343780518\n",
      "Epoch 4460 - Train Loss: 6.582245349884033 - Validation Loss: 7.412614822387695\n",
      "Epoch 4470 - Train Loss: 6.541728496551514 - Validation Loss: 7.3670573234558105\n",
      "Epoch 4480 - Train Loss: 6.50150203704834 - Validation Loss: 7.321495532989502\n",
      "Epoch 4490 - Train Loss: 6.461441993713379 - Validation Loss: 7.275980472564697\n",
      "Epoch 4500 - Train Loss: 6.420647621154785 - Validation Loss: 7.2292656898498535\n",
      "Epoch 4510 - Train Loss: 6.363344192504883 - Validation Loss: 7.159212112426758\n",
      "Epoch 4520 - Train Loss: 6.313427448272705 - Validation Loss: 7.114663124084473\n",
      "Epoch 4530 - Train Loss: 6.267455101013184 - Validation Loss: 7.068943500518799\n",
      "Epoch 4540 - Train Loss: 6.223675727844238 - Validation Loss: 7.022790431976318\n",
      "Epoch 4550 - Train Loss: 6.180230617523193 - Validation Loss: 6.976834297180176\n",
      "Epoch 4560 - Train Loss: 6.136874675750732 - Validation Loss: 6.931807994842529\n",
      "Epoch 4570 - Train Loss: 6.094499111175537 - Validation Loss: 6.888355255126953\n",
      "Epoch 4580 - Train Loss: 6.053133487701416 - Validation Loss: 6.845445156097412\n",
      "Epoch 4590 - Train Loss: 6.012267112731934 - Validation Loss: 6.803172588348389\n",
      "Epoch 4600 - Train Loss: 5.97017240524292 - Validation Loss: 6.764164447784424\n",
      "Epoch 4610 - Train Loss: 5.9284539222717285 - Validation Loss: 6.724637985229492\n",
      "Epoch 4620 - Train Loss: 5.88731575012207 - Validation Loss: 6.683682918548584\n",
      "Epoch 4630 - Train Loss: 5.847243309020996 - Validation Loss: 6.643983840942383\n",
      "Epoch 4640 - Train Loss: 5.807592391967773 - Validation Loss: 6.604284763336182\n",
      "Epoch 4650 - Train Loss: 5.767607688903809 - Validation Loss: 6.5639214515686035\n",
      "Epoch 4660 - Train Loss: 5.726306438446045 - Validation Loss: 6.5241522789001465\n",
      "Epoch 4670 - Train Loss: 5.685385227203369 - Validation Loss: 6.483567714691162\n",
      "Epoch 4680 - Train Loss: 5.644934177398682 - Validation Loss: 6.4443511962890625\n",
      "Epoch 4690 - Train Loss: 5.6052327156066895 - Validation Loss: 6.406204700469971\n",
      "Epoch 4700 - Train Loss: 5.566236972808838 - Validation Loss: 6.368351459503174\n",
      "Epoch 4710 - Train Loss: 5.528021812438965 - Validation Loss: 6.332347393035889\n",
      "Epoch 4720 - Train Loss: 5.490386962890625 - Validation Loss: 6.297319412231445\n",
      "Epoch 4730 - Train Loss: 5.45338249206543 - Validation Loss: 6.261013507843018\n",
      "Epoch 4740 - Train Loss: 5.416886806488037 - Validation Loss: 6.22635555267334\n",
      "Epoch 4750 - Train Loss: 5.3806633949279785 - Validation Loss: 6.192062854766846\n",
      "Epoch 4760 - Train Loss: 5.345035552978516 - Validation Loss: 6.156991004943848\n",
      "Epoch 4770 - Train Loss: 5.309832572937012 - Validation Loss: 6.121220588684082\n",
      "Epoch 4780 - Train Loss: 5.274713039398193 - Validation Loss: 6.084892272949219\n",
      "Epoch 4790 - Train Loss: 5.240225791931152 - Validation Loss: 6.048510551452637\n",
      "Epoch 4800 - Train Loss: 5.206374645233154 - Validation Loss: 6.0137810707092285\n",
      "Epoch 4810 - Train Loss: 5.173160552978516 - Validation Loss: 5.98036527633667\n",
      "Epoch 4820 - Train Loss: 5.140164852142334 - Validation Loss: 5.9461669921875\n",
      "Epoch 4830 - Train Loss: 5.107578277587891 - Validation Loss: 5.912352085113525\n",
      "Epoch 4840 - Train Loss: 5.075442790985107 - Validation Loss: 5.878731727600098\n",
      "Epoch 4850 - Train Loss: 5.043378829956055 - Validation Loss: 5.8449249267578125\n",
      "Epoch 4860 - Train Loss: 5.01088285446167 - Validation Loss: 5.811315536499023\n",
      "Epoch 4870 - Train Loss: 4.97818660736084 - Validation Loss: 5.777464389801025\n",
      "Epoch 4880 - Train Loss: 4.945516109466553 - Validation Loss: 5.742762088775635\n",
      "Epoch 4890 - Train Loss: 4.912766933441162 - Validation Loss: 5.70866584777832\n",
      "Epoch 4900 - Train Loss: 4.880786895751953 - Validation Loss: 5.675483703613281\n",
      "Epoch 4910 - Train Loss: 4.849374771118164 - Validation Loss: 5.643517017364502\n",
      "Epoch 4920 - Train Loss: 4.818659782409668 - Validation Loss: 5.611409664154053\n",
      "Epoch 4930 - Train Loss: 4.786816120147705 - Validation Loss: 5.57784366607666\n",
      "Epoch 4940 - Train Loss: 4.754693984985352 - Validation Loss: 5.544832706451416\n",
      "Epoch 4950 - Train Loss: 4.723352909088135 - Validation Loss: 5.51153564453125\n",
      "Epoch 4960 - Train Loss: 4.692464828491211 - Validation Loss: 5.481375694274902\n",
      "Epoch 4970 - Train Loss: 4.662164211273193 - Validation Loss: 5.451012134552002\n",
      "Epoch 4980 - Train Loss: 4.632814407348633 - Validation Loss: 5.421563148498535\n",
      "Epoch 4990 - Train Loss: 4.603442192077637 - Validation Loss: 5.390414714813232\n",
      "Epoch 5000 - Train Loss: 4.5745530128479 - Validation Loss: 5.3589043617248535\n",
      "Epoch 5010 - Train Loss: 4.5457634925842285 - Validation Loss: 5.328866481781006\n",
      "Epoch 5020 - Train Loss: 4.517437934875488 - Validation Loss: 5.299241542816162\n",
      "Epoch 5030 - Train Loss: 4.489670753479004 - Validation Loss: 5.271108150482178\n",
      "Epoch 5040 - Train Loss: 4.46241569519043 - Validation Loss: 5.242298126220703\n",
      "Epoch 5050 - Train Loss: 4.435688018798828 - Validation Loss: 5.214710235595703\n",
      "Epoch 5060 - Train Loss: 4.409608840942383 - Validation Loss: 5.186272144317627\n",
      "Epoch 5070 - Train Loss: 4.383884906768799 - Validation Loss: 5.160618782043457\n",
      "Epoch 5080 - Train Loss: 4.358487606048584 - Validation Loss: 5.133692264556885\n",
      "Epoch 5090 - Train Loss: 4.333619594573975 - Validation Loss: 5.107463836669922\n",
      "Epoch 5100 - Train Loss: 4.308982849121094 - Validation Loss: 5.082458019256592\n",
      "Epoch 5110 - Train Loss: 4.284094333648682 - Validation Loss: 5.056005477905273\n",
      "Epoch 5120 - Train Loss: 4.259629726409912 - Validation Loss: 5.031515121459961\n",
      "Epoch 5130 - Train Loss: 4.234758377075195 - Validation Loss: 5.006399154663086\n",
      "Epoch 5140 - Train Loss: 4.210391521453857 - Validation Loss: 4.981070041656494\n",
      "Epoch 5150 - Train Loss: 4.186500072479248 - Validation Loss: 4.956663608551025\n",
      "Epoch 5160 - Train Loss: 4.1627068519592285 - Validation Loss: 4.9317402839660645\n",
      "Epoch 5170 - Train Loss: 4.139025688171387 - Validation Loss: 4.907174110412598\n",
      "Epoch 5180 - Train Loss: 4.115581512451172 - Validation Loss: 4.882114410400391\n",
      "Epoch 5190 - Train Loss: 4.092535495758057 - Validation Loss: 4.856924533843994\n",
      "Epoch 5200 - Train Loss: 4.069550514221191 - Validation Loss: 4.8321943283081055\n",
      "Epoch 5210 - Train Loss: 4.046920299530029 - Validation Loss: 4.807153701782227\n",
      "Epoch 5220 - Train Loss: 4.024704456329346 - Validation Loss: 4.782683849334717\n",
      "Epoch 5230 - Train Loss: 4.00260066986084 - Validation Loss: 4.760602951049805\n",
      "Epoch 5240 - Train Loss: 3.9803667068481445 - Validation Loss: 4.737905979156494\n",
      "Epoch 5250 - Train Loss: 3.958019733428955 - Validation Loss: 4.716415882110596\n",
      "Epoch 5260 - Train Loss: 3.9349992275238037 - Validation Loss: 4.693541049957275\n",
      "Epoch 5270 - Train Loss: 3.911703586578369 - Validation Loss: 4.671077251434326\n",
      "Epoch 5280 - Train Loss: 3.8888134956359863 - Validation Loss: 4.646369934082031\n",
      "Epoch 5290 - Train Loss: 3.8642680644989014 - Validation Loss: 4.620882987976074\n",
      "Epoch 5300 - Train Loss: 3.8396356105804443 - Validation Loss: 4.590059757232666\n",
      "Epoch 5310 - Train Loss: 3.8160436153411865 - Validation Loss: 4.563538074493408\n",
      "Epoch 5320 - Train Loss: 3.793529510498047 - Validation Loss: 4.5391998291015625\n",
      "Epoch 5330 - Train Loss: 3.7711806297302246 - Validation Loss: 4.5123820304870605\n",
      "Epoch 5340 - Train Loss: 3.7497735023498535 - Validation Loss: 4.488420009613037\n",
      "Epoch 5350 - Train Loss: 3.728736400604248 - Validation Loss: 4.465207099914551\n",
      "Epoch 5360 - Train Loss: 3.708399534225464 - Validation Loss: 4.4419121742248535\n",
      "Epoch 5370 - Train Loss: 3.6881213188171387 - Validation Loss: 4.419900417327881\n",
      "Epoch 5380 - Train Loss: 3.6681575775146484 - Validation Loss: 4.398674488067627\n",
      "Epoch 5390 - Train Loss: 3.6484341621398926 - Validation Loss: 4.377510070800781\n",
      "Epoch 5400 - Train Loss: 3.6286449432373047 - Validation Loss: 4.356836318969727\n",
      "Epoch 5410 - Train Loss: 3.6093926429748535 - Validation Loss: 4.335727691650391\n",
      "Epoch 5420 - Train Loss: 3.5906169414520264 - Validation Loss: 4.314342021942139\n",
      "Epoch 5430 - Train Loss: 3.5708084106445312 - Validation Loss: 4.2930779457092285\n",
      "Epoch 5440 - Train Loss: 3.5514655113220215 - Validation Loss: 4.270663738250732\n",
      "Epoch 5450 - Train Loss: 3.5321435928344727 - Validation Loss: 4.247999668121338\n",
      "Epoch 5460 - Train Loss: 3.5118298530578613 - Validation Loss: 4.226221084594727\n",
      "Epoch 5470 - Train Loss: 3.491785764694214 - Validation Loss: 4.201334476470947\n",
      "Epoch 5480 - Train Loss: 3.47214412689209 - Validation Loss: 4.178678512573242\n",
      "Epoch 5490 - Train Loss: 3.4523308277130127 - Validation Loss: 4.155604839324951\n",
      "Epoch 5500 - Train Loss: 3.432098627090454 - Validation Loss: 4.132893085479736\n",
      "Epoch 5510 - Train Loss: 3.4125452041625977 - Validation Loss: 4.109974384307861\n",
      "Epoch 5520 - Train Loss: 3.3935904502868652 - Validation Loss: 4.089907646179199\n",
      "Epoch 5530 - Train Loss: 3.3738741874694824 - Validation Loss: 4.0667243003845215\n",
      "Epoch 5540 - Train Loss: 3.3533248901367188 - Validation Loss: 4.045226097106934\n",
      "Epoch 5550 - Train Loss: 3.332557201385498 - Validation Loss: 4.023252964019775\n",
      "Epoch 5560 - Train Loss: 3.3110029697418213 - Validation Loss: 3.9987733364105225\n",
      "Epoch 5570 - Train Loss: 3.29024076461792 - Validation Loss: 3.974947452545166\n",
      "Epoch 5580 - Train Loss: 3.2693076133728027 - Validation Loss: 3.951226234436035\n",
      "Epoch 5590 - Train Loss: 3.2490432262420654 - Validation Loss: 3.928675651550293\n",
      "Epoch 5600 - Train Loss: 3.2291126251220703 - Validation Loss: 3.90812349319458\n",
      "Epoch 5610 - Train Loss: 3.209336519241333 - Validation Loss: 3.8865320682525635\n",
      "Epoch 5620 - Train Loss: 3.189847230911255 - Validation Loss: 3.866018056869507\n",
      "Epoch 5630 - Train Loss: 3.169990062713623 - Validation Loss: 3.8456127643585205\n",
      "Epoch 5640 - Train Loss: 3.1499710083007812 - Validation Loss: 3.8253729343414307\n",
      "Epoch 5650 - Train Loss: 3.12984561920166 - Validation Loss: 3.8047163486480713\n",
      "Epoch 5660 - Train Loss: 3.110024929046631 - Validation Loss: 3.7849385738372803\n",
      "Epoch 5670 - Train Loss: 3.090581178665161 - Validation Loss: 3.764124870300293\n",
      "Epoch 5680 - Train Loss: 3.0716187953948975 - Validation Loss: 3.7442078590393066\n",
      "Epoch 5690 - Train Loss: 3.0527327060699463 - Validation Loss: 3.7256739139556885\n",
      "Epoch 5700 - Train Loss: 3.033644914627075 - Validation Loss: 3.7046291828155518\n",
      "Epoch 5710 - Train Loss: 3.015254259109497 - Validation Loss: 3.6848368644714355\n",
      "Epoch 5720 - Train Loss: 2.9965343475341797 - Validation Loss: 3.6574878692626953\n",
      "Epoch 5730 - Train Loss: 2.9782211780548096 - Validation Loss: 3.6335601806640625\n",
      "Epoch 5740 - Train Loss: 2.9603500366210938 - Validation Loss: 3.610902786254883\n",
      "Epoch 5750 - Train Loss: 2.942739725112915 - Validation Loss: 3.590088367462158\n",
      "Epoch 5760 - Train Loss: 2.9258928298950195 - Validation Loss: 3.566746711730957\n",
      "Epoch 5770 - Train Loss: 2.9088761806488037 - Validation Loss: 3.5479788780212402\n",
      "Epoch 5780 - Train Loss: 2.8918161392211914 - Validation Loss: 3.528435230255127\n",
      "Epoch 5790 - Train Loss: 2.875202178955078 - Validation Loss: 3.5090487003326416\n",
      "Epoch 5800 - Train Loss: 2.8586578369140625 - Validation Loss: 3.4906044006347656\n",
      "Epoch 5810 - Train Loss: 2.8425040245056152 - Validation Loss: 3.472688674926758\n",
      "Epoch 5820 - Train Loss: 2.8262104988098145 - Validation Loss: 3.4543616771698\n",
      "Epoch 5830 - Train Loss: 2.8100106716156006 - Validation Loss: 3.4373815059661865\n",
      "Epoch 5840 - Train Loss: 2.7940011024475098 - Validation Loss: 3.418977975845337\n",
      "Epoch 5850 - Train Loss: 2.7781894207000732 - Validation Loss: 3.401303291320801\n",
      "Epoch 5860 - Train Loss: 2.762664318084717 - Validation Loss: 3.3834991455078125\n",
      "Epoch 5870 - Train Loss: 2.7471792697906494 - Validation Loss: 3.3652312755584717\n",
      "Epoch 5880 - Train Loss: 2.731815814971924 - Validation Loss: 3.3500335216522217\n",
      "Epoch 5890 - Train Loss: 2.7166218757629395 - Validation Loss: 3.3323614597320557\n",
      "Epoch 5900 - Train Loss: 2.7021427154541016 - Validation Loss: 3.316157102584839\n",
      "Epoch 5910 - Train Loss: 2.6875381469726562 - Validation Loss: 3.302525520324707\n",
      "Epoch 5920 - Train Loss: 2.672910690307617 - Validation Loss: 3.2863597869873047\n",
      "Epoch 5930 - Train Loss: 2.658399820327759 - Validation Loss: 3.2722997665405273\n",
      "Epoch 5940 - Train Loss: 2.6441047191619873 - Validation Loss: 3.2576987743377686\n",
      "Epoch 5950 - Train Loss: 2.629706382751465 - Validation Loss: 3.2430901527404785\n",
      "Epoch 5960 - Train Loss: 2.6148860454559326 - Validation Loss: 3.2276389598846436\n",
      "Epoch 5970 - Train Loss: 2.6006953716278076 - Validation Loss: 3.2139735221862793\n",
      "Epoch 5980 - Train Loss: 2.5865700244903564 - Validation Loss: 3.199863910675049\n",
      "Epoch 5990 - Train Loss: 2.5726115703582764 - Validation Loss: 3.184736967086792\n",
      "Epoch 6000 - Train Loss: 2.558361768722534 - Validation Loss: 3.171337366104126\n",
      "Epoch 6010 - Train Loss: 2.5441033840179443 - Validation Loss: 3.1554629802703857\n",
      "Epoch 6020 - Train Loss: 2.529677629470825 - Validation Loss: 3.141709089279175\n",
      "Epoch 6030 - Train Loss: 2.5153677463531494 - Validation Loss: 3.1253488063812256\n",
      "Epoch 6040 - Train Loss: 2.5013132095336914 - Validation Loss: 3.110219717025757\n",
      "Epoch 6050 - Train Loss: 2.48711895942688 - Validation Loss: 3.0953431129455566\n",
      "Epoch 6060 - Train Loss: 2.4727413654327393 - Validation Loss: 3.08107852935791\n",
      "Epoch 6070 - Train Loss: 2.457880735397339 - Validation Loss: 3.065416097640991\n",
      "Epoch 6080 - Train Loss: 2.443690299987793 - Validation Loss: 3.0525784492492676\n",
      "Epoch 6090 - Train Loss: 2.429831027984619 - Validation Loss: 3.03653621673584\n",
      "Epoch 6100 - Train Loss: 2.4167380332946777 - Validation Loss: 3.0216519832611084\n",
      "Epoch 6110 - Train Loss: 2.403351068496704 - Validation Loss: 3.0065371990203857\n",
      "Epoch 6120 - Train Loss: 2.3902504444122314 - Validation Loss: 2.993068218231201\n",
      "Epoch 6130 - Train Loss: 2.377492904663086 - Validation Loss: 2.978182554244995\n",
      "Epoch 6140 - Train Loss: 2.3643438816070557 - Validation Loss: 2.963632583618164\n",
      "Epoch 6150 - Train Loss: 2.3518567085266113 - Validation Loss: 2.9497122764587402\n",
      "Epoch 6160 - Train Loss: 2.3398122787475586 - Validation Loss: 2.936119556427002\n",
      "Epoch 6170 - Train Loss: 2.3272275924682617 - Validation Loss: 2.921844959259033\n",
      "Epoch 6180 - Train Loss: 2.3150861263275146 - Validation Loss: 2.9084622859954834\n",
      "Epoch 6190 - Train Loss: 2.303846836090088 - Validation Loss: 2.895250082015991\n",
      "Epoch 6200 - Train Loss: 2.290938377380371 - Validation Loss: 2.8798677921295166\n",
      "Epoch 6210 - Train Loss: 2.2787041664123535 - Validation Loss: 2.867305040359497\n",
      "Epoch 6220 - Train Loss: 2.266772508621216 - Validation Loss: 2.854492425918579\n",
      "Epoch 6230 - Train Loss: 2.2549428939819336 - Validation Loss: 2.8393969535827637\n",
      "Epoch 6240 - Train Loss: 2.2431817054748535 - Validation Loss: 2.8260891437530518\n",
      "Epoch 6250 - Train Loss: 2.231926441192627 - Validation Loss: 2.813235282897949\n",
      "Epoch 6260 - Train Loss: 2.2208268642425537 - Validation Loss: 2.8005430698394775\n",
      "Epoch 6270 - Train Loss: 2.2095134258270264 - Validation Loss: 2.786008358001709\n",
      "Epoch 6280 - Train Loss: 2.1986875534057617 - Validation Loss: 2.773263931274414\n",
      "Epoch 6290 - Train Loss: 2.186688184738159 - Validation Loss: 2.7571842670440674\n",
      "Epoch 6300 - Train Loss: 2.175771951675415 - Validation Loss: 2.7448136806488037\n",
      "Epoch 6310 - Train Loss: 2.164241075515747 - Validation Loss: 2.730860710144043\n",
      "Epoch 6320 - Train Loss: 2.1538894176483154 - Validation Loss: 2.7202789783477783\n",
      "Epoch 6330 - Train Loss: 2.1424131393432617 - Validation Loss: 2.705662488937378\n",
      "Epoch 6340 - Train Loss: 2.1319048404693604 - Validation Loss: 2.6939947605133057\n",
      "Epoch 6350 - Train Loss: 2.120980739593506 - Validation Loss: 2.6827681064605713\n",
      "Epoch 6360 - Train Loss: 2.110013723373413 - Validation Loss: 2.669196605682373\n",
      "Epoch 6370 - Train Loss: 2.0996410846710205 - Validation Loss: 2.6565818786621094\n",
      "Epoch 6380 - Train Loss: 2.0885491371154785 - Validation Loss: 2.644228219985962\n",
      "Epoch 6390 - Train Loss: 2.0774776935577393 - Validation Loss: 2.6302714347839355\n",
      "Epoch 6400 - Train Loss: 2.0667800903320312 - Validation Loss: 2.61785888671875\n",
      "Epoch 6410 - Train Loss: 2.0554771423339844 - Validation Loss: 2.6044609546661377\n",
      "Epoch 6420 - Train Loss: 2.044729471206665 - Validation Loss: 2.591884136199951\n",
      "Epoch 6430 - Train Loss: 2.034107208251953 - Validation Loss: 2.5775561332702637\n",
      "Epoch 6440 - Train Loss: 2.023184299468994 - Validation Loss: 2.565706968307495\n",
      "Epoch 6450 - Train Loss: 2.012789249420166 - Validation Loss: 2.554149866104126\n",
      "Epoch 6460 - Train Loss: 2.0022361278533936 - Validation Loss: 2.5409293174743652\n",
      "Epoch 6470 - Train Loss: 1.9914417266845703 - Validation Loss: 2.5270464420318604\n",
      "Epoch 6480 - Train Loss: 1.9812260866165161 - Validation Loss: 2.5129995346069336\n",
      "Epoch 6490 - Train Loss: 1.9707609415054321 - Validation Loss: 2.5009617805480957\n",
      "Epoch 6500 - Train Loss: 1.9610873460769653 - Validation Loss: 2.490457057952881\n",
      "Epoch 6510 - Train Loss: 1.950556755065918 - Validation Loss: 2.478217840194702\n",
      "Epoch 6520 - Train Loss: 1.9404786825180054 - Validation Loss: 2.467588186264038\n",
      "Epoch 6530 - Train Loss: 1.9304225444793701 - Validation Loss: 2.4557178020477295\n",
      "Epoch 6540 - Train Loss: 1.9203850030899048 - Validation Loss: 2.445152997970581\n",
      "Epoch 6550 - Train Loss: 1.9104496240615845 - Validation Loss: 2.434389352798462\n",
      "Epoch 6560 - Train Loss: 1.9009126424789429 - Validation Loss: 2.424806594848633\n",
      "Epoch 6570 - Train Loss: 1.891575574874878 - Validation Loss: 2.413041591644287\n",
      "Epoch 6580 - Train Loss: 1.8807247877120972 - Validation Loss: 2.399012327194214\n",
      "Epoch 6590 - Train Loss: 1.869838833808899 - Validation Loss: 2.3869059085845947\n",
      "Epoch 6600 - Train Loss: 1.8584059476852417 - Validation Loss: 2.372067928314209\n",
      "Epoch 6610 - Train Loss: 1.848050594329834 - Validation Loss: 2.3601067066192627\n",
      "Epoch 6620 - Train Loss: 1.8377807140350342 - Validation Loss: 2.348072052001953\n",
      "Epoch 6630 - Train Loss: 1.8283287286758423 - Validation Loss: 2.339367151260376\n",
      "Epoch 6640 - Train Loss: 1.819115161895752 - Validation Loss: 2.3279380798339844\n",
      "Epoch 6650 - Train Loss: 1.8095166683197021 - Validation Loss: 2.316596508026123\n",
      "Epoch 6660 - Train Loss: 1.8011839389801025 - Validation Loss: 2.3065173625946045\n",
      "Epoch 6670 - Train Loss: 1.7921431064605713 - Validation Loss: 2.2958219051361084\n",
      "Epoch 6680 - Train Loss: 1.7841746807098389 - Validation Loss: 2.2890067100524902\n",
      "Epoch 6690 - Train Loss: 1.7746264934539795 - Validation Loss: 2.2763559818267822\n",
      "Epoch 6700 - Train Loss: 1.7667022943496704 - Validation Loss: 2.266105890274048\n",
      "Epoch 6710 - Train Loss: 1.7578275203704834 - Validation Loss: 2.258551836013794\n",
      "Epoch 6720 - Train Loss: 1.7502741813659668 - Validation Loss: 2.2507262229919434\n",
      "Epoch 6730 - Train Loss: 1.7421135902404785 - Validation Loss: 2.2408127784729004\n",
      "Epoch 6740 - Train Loss: 1.7337324619293213 - Validation Loss: 2.230897903442383\n",
      "Epoch 6750 - Train Loss: 1.7259612083435059 - Validation Loss: 2.224242925643921\n",
      "Epoch 6760 - Train Loss: 1.7181241512298584 - Validation Loss: 2.214921474456787\n",
      "Epoch 6770 - Train Loss: 1.7109521627426147 - Validation Loss: 2.20548939704895\n",
      "Epoch 6780 - Train Loss: 1.7027430534362793 - Validation Loss: 2.198192596435547\n",
      "Epoch 6790 - Train Loss: 1.695684790611267 - Validation Loss: 2.190980911254883\n",
      "Epoch 6800 - Train Loss: 1.6886111497879028 - Validation Loss: 2.1824917793273926\n",
      "Epoch 6810 - Train Loss: 1.6810721158981323 - Validation Loss: 2.171036958694458\n",
      "Epoch 6820 - Train Loss: 1.6735605001449585 - Validation Loss: 2.1652607917785645\n",
      "Epoch 6830 - Train Loss: 1.6663252115249634 - Validation Loss: 2.154390573501587\n",
      "Epoch 6840 - Train Loss: 1.6584968566894531 - Validation Loss: 2.147658586502075\n",
      "Epoch 6850 - Train Loss: 1.6516164541244507 - Validation Loss: 2.140913963317871\n",
      "Epoch 6860 - Train Loss: 1.6444162130355835 - Validation Loss: 2.130596160888672\n",
      "Epoch 6870 - Train Loss: 1.6374199390411377 - Validation Loss: 2.122819185256958\n",
      "Epoch 6880 - Train Loss: 1.6295865774154663 - Validation Loss: 2.1136388778686523\n",
      "Epoch 6890 - Train Loss: 1.622673511505127 - Validation Loss: 2.104975461959839\n",
      "Epoch 6900 - Train Loss: 1.616135597229004 - Validation Loss: 2.0971803665161133\n",
      "Epoch 6910 - Train Loss: 1.6083050966262817 - Validation Loss: 2.091550588607788\n",
      "Epoch 6920 - Train Loss: 1.6015015840530396 - Validation Loss: 2.082101345062256\n",
      "Epoch 6930 - Train Loss: 1.5948455333709717 - Validation Loss: 2.0724847316741943\n",
      "Epoch 6940 - Train Loss: 1.5878225564956665 - Validation Loss: 2.0680387020111084\n",
      "Epoch 6950 - Train Loss: 1.5803838968276978 - Validation Loss: 2.0568172931671143\n",
      "Epoch 6960 - Train Loss: 1.5728132724761963 - Validation Loss: 2.0504860877990723\n",
      "Epoch 6970 - Train Loss: 1.5661085844039917 - Validation Loss: 2.042736768722534\n",
      "Epoch 6980 - Train Loss: 1.5603727102279663 - Validation Loss: 2.033604383468628\n",
      "Epoch 6990 - Train Loss: 1.5538716316223145 - Validation Loss: 2.0301737785339355\n",
      "Epoch 7000 - Train Loss: 1.5470384359359741 - Validation Loss: 2.017739772796631\n",
      "Epoch 7010 - Train Loss: 1.5401862859725952 - Validation Loss: 2.0146000385284424\n",
      "Epoch 7020 - Train Loss: 1.5339254140853882 - Validation Loss: 2.0035855770111084\n",
      "Epoch 7030 - Train Loss: 1.5268272161483765 - Validation Loss: 1.9978488683700562\n",
      "Epoch 7040 - Train Loss: 1.5205516815185547 - Validation Loss: 1.9893728494644165\n",
      "Epoch 7050 - Train Loss: 1.5140937566757202 - Validation Loss: 1.982070803642273\n",
      "Epoch 7060 - Train Loss: 1.5077608823776245 - Validation Loss: 1.975406527519226\n",
      "Epoch 7070 - Train Loss: 1.501665472984314 - Validation Loss: 1.9670354127883911\n",
      "Epoch 7080 - Train Loss: 1.4962043762207031 - Validation Loss: 1.960609793663025\n",
      "Epoch 7090 - Train Loss: 1.490191102027893 - Validation Loss: 1.9522194862365723\n",
      "Epoch 7100 - Train Loss: 1.484046459197998 - Validation Loss: 1.9443726539611816\n",
      "Epoch 7110 - Train Loss: 1.4795520305633545 - Validation Loss: 1.9402258396148682\n",
      "Epoch 7120 - Train Loss: 1.4745427370071411 - Validation Loss: 1.9327694177627563\n",
      "Epoch 7130 - Train Loss: 1.4676513671875 - Validation Loss: 1.9241011142730713\n",
      "Epoch 7140 - Train Loss: 1.4637404680252075 - Validation Loss: 1.9161193370819092\n",
      "Epoch 7150 - Train Loss: 1.457091212272644 - Validation Loss: 1.91278076171875\n",
      "Epoch 7160 - Train Loss: 1.4513869285583496 - Validation Loss: 1.9031184911727905\n",
      "Epoch 7170 - Train Loss: 1.446399450302124 - Validation Loss: 1.8959664106369019\n",
      "Epoch 7180 - Train Loss: 1.440812349319458 - Validation Loss: 1.8901528120040894\n",
      "Epoch 7190 - Train Loss: 1.43651282787323 - Validation Loss: 1.8862566947937012\n",
      "Epoch 7200 - Train Loss: 1.4309618473052979 - Validation Loss: 1.8776987791061401\n",
      "Epoch 7210 - Train Loss: 1.4276165962219238 - Validation Loss: 1.8707866668701172\n",
      "Epoch 7220 - Train Loss: 1.4206210374832153 - Validation Loss: 1.8663591146469116\n",
      "Epoch 7230 - Train Loss: 1.4157366752624512 - Validation Loss: 1.858510971069336\n",
      "Epoch 7240 - Train Loss: 1.4113353490829468 - Validation Loss: 1.850986361503601\n",
      "Epoch 7250 - Train Loss: 1.4071404933929443 - Validation Loss: 1.8480606079101562\n",
      "Epoch 7260 - Train Loss: 1.4018694162368774 - Validation Loss: 1.837068796157837\n",
      "Epoch 7270 - Train Loss: 1.3965625762939453 - Validation Loss: 1.8330134153366089\n",
      "Epoch 7280 - Train Loss: 1.391802430152893 - Validation Loss: 1.824446201324463\n",
      "Epoch 7290 - Train Loss: 1.387574315071106 - Validation Loss: 1.817490816116333\n",
      "Epoch 7300 - Train Loss: 1.382790207862854 - Validation Loss: 1.8105125427246094\n",
      "Epoch 7310 - Train Loss: 1.378199577331543 - Validation Loss: 1.8070216178894043\n",
      "Epoch 7320 - Train Loss: 1.372329831123352 - Validation Loss: 1.7962394952774048\n",
      "Epoch 7330 - Train Loss: 1.3671221733093262 - Validation Loss: 1.7893164157867432\n",
      "Epoch 7340 - Train Loss: 1.3626859188079834 - Validation Loss: 1.785239577293396\n",
      "Epoch 7350 - Train Loss: 1.357636570930481 - Validation Loss: 1.7768244743347168\n",
      "Epoch 7360 - Train Loss: 1.353562355041504 - Validation Loss: 1.7689111232757568\n",
      "Epoch 7370 - Train Loss: 1.3482896089553833 - Validation Loss: 1.7640458345413208\n",
      "Epoch 7380 - Train Loss: 1.3437923192977905 - Validation Loss: 1.7583268880844116\n",
      "Epoch 7390 - Train Loss: 1.3396589756011963 - Validation Loss: 1.750229001045227\n",
      "Epoch 7400 - Train Loss: 1.3341097831726074 - Validation Loss: 1.746225118637085\n",
      "Epoch 7410 - Train Loss: 1.329700231552124 - Validation Loss: 1.740053653717041\n",
      "Epoch 7420 - Train Loss: 1.3255131244659424 - Validation Loss: 1.7333319187164307\n",
      "Epoch 7430 - Train Loss: 1.3207629919052124 - Validation Loss: 1.7293460369110107\n",
      "Epoch 7440 - Train Loss: 1.3156623840332031 - Validation Loss: 1.7236217260360718\n",
      "Epoch 7450 - Train Loss: 1.3121007680892944 - Validation Loss: 1.7187308073043823\n",
      "Epoch 7460 - Train Loss: 1.3067333698272705 - Validation Loss: 1.7133121490478516\n",
      "Epoch 7470 - Train Loss: 1.3033523559570312 - Validation Loss: 1.7114624977111816\n",
      "Epoch 7480 - Train Loss: 1.2975459098815918 - Validation Loss: 1.7019219398498535\n",
      "Epoch 7490 - Train Loss: 1.2940789461135864 - Validation Loss: 1.6970199346542358\n",
      "Epoch 7500 - Train Loss: 1.2891302108764648 - Validation Loss: 1.6939067840576172\n",
      "Epoch 7510 - Train Loss: 1.2859171628952026 - Validation Loss: 1.687793254852295\n",
      "Epoch 7520 - Train Loss: 1.2807326316833496 - Validation Loss: 1.6808669567108154\n",
      "Epoch 7530 - Train Loss: 1.276700496673584 - Validation Loss: 1.6752548217773438\n",
      "Epoch 7540 - Train Loss: 1.272847294807434 - Validation Loss: 1.674001693725586\n",
      "Epoch 7550 - Train Loss: 1.2676091194152832 - Validation Loss: 1.665108561515808\n",
      "Epoch 7560 - Train Loss: 1.2637159824371338 - Validation Loss: 1.6598783731460571\n",
      "Epoch 7570 - Train Loss: 1.2592964172363281 - Validation Loss: 1.6547330617904663\n",
      "Epoch 7580 - Train Loss: 1.2580782175064087 - Validation Loss: 1.6532944440841675\n",
      "Epoch 7590 - Train Loss: 1.2521682977676392 - Validation Loss: 1.6434966325759888\n",
      "Epoch 7600 - Train Loss: 1.2477397918701172 - Validation Loss: 1.6387869119644165\n",
      "Epoch 7610 - Train Loss: 1.2450110912322998 - Validation Loss: 1.6348565816879272\n",
      "Epoch 7620 - Train Loss: 1.2398922443389893 - Validation Loss: 1.6295090913772583\n",
      "Epoch 7630 - Train Loss: 1.23508620262146 - Validation Loss: 1.623611330986023\n",
      "Epoch 7640 - Train Loss: 1.230902075767517 - Validation Loss: 1.6174566745758057\n",
      "Epoch 7650 - Train Loss: 1.2259639501571655 - Validation Loss: 1.6085528135299683\n",
      "Epoch 7660 - Train Loss: 1.2220405340194702 - Validation Loss: 1.6021521091461182\n",
      "Epoch 7670 - Train Loss: 1.21721613407135 - Validation Loss: 1.5981463193893433\n",
      "Epoch 7680 - Train Loss: 1.212695837020874 - Validation Loss: 1.5907411575317383\n",
      "Epoch 7690 - Train Loss: 1.2083911895751953 - Validation Loss: 1.5859285593032837\n",
      "Epoch 7700 - Train Loss: 1.205183982849121 - Validation Loss: 1.5832605361938477\n",
      "Epoch 7710 - Train Loss: 1.200022578239441 - Validation Loss: 1.5764193534851074\n",
      "Epoch 7720 - Train Loss: 1.1963239908218384 - Validation Loss: 1.5727479457855225\n",
      "Epoch 7730 - Train Loss: 1.1933115720748901 - Validation Loss: 1.5688602924346924\n",
      "Epoch 7740 - Train Loss: 1.1898019313812256 - Validation Loss: 1.5628246068954468\n",
      "Epoch 7750 - Train Loss: 1.1844269037246704 - Validation Loss: 1.5590828657150269\n",
      "Epoch 7760 - Train Loss: 1.180594801902771 - Validation Loss: 1.5536551475524902\n",
      "Epoch 7770 - Train Loss: 1.1766096353530884 - Validation Loss: 1.5469566583633423\n",
      "Epoch 7780 - Train Loss: 1.17135751247406 - Validation Loss: 1.5385199785232544\n",
      "Epoch 7790 - Train Loss: 1.1680362224578857 - Validation Loss: 1.5328282117843628\n",
      "Epoch 7800 - Train Loss: 1.1623915433883667 - Validation Loss: 1.5282196998596191\n",
      "Epoch 7810 - Train Loss: 1.160229206085205 - Validation Loss: 1.527218222618103\n",
      "Epoch 7820 - Train Loss: 1.1546893119812012 - Validation Loss: 1.518846035003662\n",
      "Epoch 7830 - Train Loss: 1.1525355577468872 - Validation Loss: 1.514101266860962\n",
      "Epoch 7840 - Train Loss: 1.147423505783081 - Validation Loss: 1.5101770162582397\n",
      "Epoch 7850 - Train Loss: 1.145763635635376 - Validation Loss: 1.5099514722824097\n",
      "Epoch 7860 - Train Loss: 1.1401150226593018 - Validation Loss: 1.5005652904510498\n",
      "Epoch 7870 - Train Loss: 1.1381802558898926 - Validation Loss: 1.4959006309509277\n",
      "Epoch 7880 - Train Loss: 1.1331039667129517 - Validation Loss: 1.4932862520217896\n",
      "Epoch 7890 - Train Loss: 1.1302226781845093 - Validation Loss: 1.490044116973877\n",
      "Epoch 7900 - Train Loss: 1.1257805824279785 - Validation Loss: 1.4829214811325073\n",
      "Epoch 7910 - Train Loss: 1.1237932443618774 - Validation Loss: 1.482909917831421\n",
      "Epoch 7920 - Train Loss: 1.118903398513794 - Validation Loss: 1.4747700691223145\n",
      "Epoch 7930 - Train Loss: 1.11665678024292 - Validation Loss: 1.4698094129562378\n",
      "Epoch 7940 - Train Loss: 1.11260986328125 - Validation Loss: 1.4674639701843262\n",
      "Epoch 7950 - Train Loss: 1.1091587543487549 - Validation Loss: 1.4626731872558594\n",
      "Epoch 7960 - Train Loss: 1.1051121950149536 - Validation Loss: 1.4553632736206055\n",
      "Epoch 7970 - Train Loss: 1.1069812774658203 - Validation Loss: 1.452291488647461\n",
      "Epoch 7980 - Train Loss: 1.0993729829788208 - Validation Loss: 1.4510767459869385\n",
      "Epoch 7990 - Train Loss: 1.094813346862793 - Validation Loss: 1.44256591796875\n",
      "Epoch 8000 - Train Loss: 1.0913727283477783 - Validation Loss: 1.4374632835388184\n",
      "Epoch 8010 - Train Loss: 1.089662790298462 - Validation Loss: 1.4347847700119019\n",
      "Epoch 8020 - Train Loss: 1.084216833114624 - Validation Loss: 1.4295886754989624\n",
      "Epoch 8030 - Train Loss: 1.0830249786376953 - Validation Loss: 1.428722858428955\n",
      "Epoch 8040 - Train Loss: 1.0776604413986206 - Validation Loss: 1.4209173917770386\n",
      "Epoch 8050 - Train Loss: 1.0749998092651367 - Validation Loss: 1.4164797067642212\n",
      "Epoch 8060 - Train Loss: 1.071086049079895 - Validation Loss: 1.413287878036499\n",
      "Epoch 8070 - Train Loss: 1.0689557790756226 - Validation Loss: 1.4114545583724976\n",
      "Epoch 8080 - Train Loss: 1.064881682395935 - Validation Loss: 1.4056055545806885\n",
      "Epoch 8090 - Train Loss: 1.0614356994628906 - Validation Loss: 1.4015120267868042\n",
      "Epoch 8100 - Train Loss: 1.059302806854248 - Validation Loss: 1.398022174835205\n",
      "Epoch 8110 - Train Loss: 1.0555715560913086 - Validation Loss: 1.391735553741455\n",
      "Epoch 8120 - Train Loss: 1.0519434213638306 - Validation Loss: 1.3875343799591064\n",
      "Epoch 8130 - Train Loss: 1.048775553703308 - Validation Loss: 1.3858273029327393\n",
      "Epoch 8140 - Train Loss: 1.0464749336242676 - Validation Loss: 1.3811864852905273\n",
      "Epoch 8150 - Train Loss: 1.043827772140503 - Validation Loss: 1.3760030269622803\n",
      "Epoch 8160 - Train Loss: 1.0404223203659058 - Validation Loss: 1.3706326484680176\n",
      "Epoch 8170 - Train Loss: 1.036469578742981 - Validation Loss: 1.3686751127243042\n",
      "Epoch 8180 - Train Loss: 1.0334800481796265 - Validation Loss: 1.365708589553833\n",
      "Epoch 8190 - Train Loss: 1.0298168659210205 - Validation Loss: 1.3604533672332764\n",
      "Epoch 8200 - Train Loss: 1.0276366472244263 - Validation Loss: 1.3588130474090576\n",
      "Epoch 8210 - Train Loss: 1.0237747430801392 - Validation Loss: 1.3523638248443604\n",
      "Epoch 8220 - Train Loss: 1.0208224058151245 - Validation Loss: 1.3469313383102417\n",
      "Epoch 8230 - Train Loss: 1.0192084312438965 - Validation Loss: 1.343446135520935\n",
      "Epoch 8240 - Train Loss: 1.0152404308319092 - Validation Loss: 1.3419502973556519\n",
      "Epoch 8250 - Train Loss: 1.011580467224121 - Validation Loss: 1.3358250856399536\n",
      "Epoch 8260 - Train Loss: 1.0095009803771973 - Validation Loss: 1.3307764530181885\n",
      "Epoch 8270 - Train Loss: 1.0051958560943604 - Validation Loss: 1.3261562585830688\n",
      "Epoch 8280 - Train Loss: 1.0030208826065063 - Validation Loss: 1.3257092237472534\n",
      "Epoch 8290 - Train Loss: 0.9998546242713928 - Validation Loss: 1.3197245597839355\n",
      "Epoch 8300 - Train Loss: 0.9973410367965698 - Validation Loss: 1.3147512674331665\n",
      "Epoch 8310 - Train Loss: 0.9954325556755066 - Validation Loss: 1.3107240200042725\n",
      "Epoch 8320 - Train Loss: 0.9906764030456543 - Validation Loss: 1.307027816772461\n",
      "Epoch 8330 - Train Loss: 0.9889107942581177 - Validation Loss: 1.3061144351959229\n",
      "Epoch 8340 - Train Loss: 0.9853575229644775 - Validation Loss: 1.3009308576583862\n",
      "Epoch 8350 - Train Loss: 0.9830325245857239 - Validation Loss: 1.2948538064956665\n",
      "Epoch 8360 - Train Loss: 0.9799712896347046 - Validation Loss: 1.2904257774353027\n",
      "Epoch 8370 - Train Loss: 0.9757971167564392 - Validation Loss: 1.288151502609253\n",
      "Epoch 8380 - Train Loss: 0.9748957753181458 - Validation Loss: 1.2866865396499634\n",
      "Epoch 8390 - Train Loss: 0.9704632759094238 - Validation Loss: 1.2786906957626343\n",
      "Epoch 8400 - Train Loss: 0.9682950377464294 - Validation Loss: 1.274734377861023\n",
      "Epoch 8410 - Train Loss: 0.9640116095542908 - Validation Loss: 1.2697495222091675\n",
      "Epoch 8420 - Train Loss: 0.9617676734924316 - Validation Loss: 1.2687047719955444\n",
      "Epoch 8430 - Train Loss: 0.9590031504631042 - Validation Loss: 1.2653974294662476\n",
      "Epoch 8440 - Train Loss: 0.9556423425674438 - Validation Loss: 1.2597606182098389\n",
      "Epoch 8450 - Train Loss: 0.9522230625152588 - Validation Loss: 1.2569471597671509\n",
      "Epoch 8460 - Train Loss: 0.9513381123542786 - Validation Loss: 1.2544947862625122\n",
      "Epoch 8470 - Train Loss: 0.947626531124115 - Validation Loss: 1.2478430271148682\n",
      "Epoch 8480 - Train Loss: 0.943466305732727 - Validation Loss: 1.2428144216537476\n",
      "Epoch 8490 - Train Loss: 0.9402151703834534 - Validation Loss: 1.2405095100402832\n",
      "Epoch 8500 - Train Loss: 0.9405820369720459 - Validation Loss: 1.2414690256118774\n",
      "Epoch 8510 - Train Loss: 0.9340780377388 - Validation Loss: 1.2300111055374146\n",
      "Epoch 8520 - Train Loss: 0.9338836073875427 - Validation Loss: 1.2276442050933838\n",
      "Epoch 8530 - Train Loss: 0.9293671250343323 - Validation Loss: 1.2274538278579712\n",
      "Epoch 8540 - Train Loss: 0.9253008961677551 - Validation Loss: 1.2194557189941406\n",
      "Epoch 8550 - Train Loss: 0.924149215221405 - Validation Loss: 1.2154171466827393\n",
      "Epoch 8560 - Train Loss: 0.9203201532363892 - Validation Loss: 1.214650273323059\n",
      "Epoch 8570 - Train Loss: 0.9178194999694824 - Validation Loss: 1.211186408996582\n",
      "Epoch 8580 - Train Loss: 0.9141960740089417 - Validation Loss: 1.2038813829421997\n",
      "Epoch 8590 - Train Loss: 0.9131165742874146 - Validation Loss: 1.2019580602645874\n",
      "Epoch 8600 - Train Loss: 0.9099875092506409 - Validation Loss: 1.2017048597335815\n",
      "Epoch 8610 - Train Loss: 0.9056851267814636 - Validation Loss: 1.1936918497085571\n",
      "Epoch 8620 - Train Loss: 0.9026046395301819 - Validation Loss: 1.190216302871704\n",
      "Epoch 8630 - Train Loss: 0.9008133411407471 - Validation Loss: 1.191756010055542\n",
      "Epoch 8640 - Train Loss: 0.8959681391716003 - Validation Loss: 1.1832302808761597\n",
      "Epoch 8650 - Train Loss: 0.8955197334289551 - Validation Loss: 1.1807856559753418\n",
      "Epoch 8660 - Train Loss: 0.8918836116790771 - Validation Loss: 1.1807162761688232\n",
      "Epoch 8670 - Train Loss: 0.8882333040237427 - Validation Loss: 1.174765706062317\n",
      "Epoch 8680 - Train Loss: 0.8863850831985474 - Validation Loss: 1.1723747253417969\n",
      "Epoch 8690 - Train Loss: 0.8827283382415771 - Validation Loss: 1.1713645458221436\n",
      "Epoch 8700 - Train Loss: 0.8794105648994446 - Validation Loss: 1.1655253171920776\n",
      "Epoch 8710 - Train Loss: 0.8770813345909119 - Validation Loss: 1.161891222000122\n",
      "Epoch 8720 - Train Loss: 0.8728238940238953 - Validation Loss: 1.1572989225387573\n",
      "Epoch 8730 - Train Loss: 0.8694049119949341 - Validation Loss: 1.1553146839141846\n",
      "Epoch 8740 - Train Loss: 0.8685371279716492 - Validation Loss: 1.1537628173828125\n",
      "Epoch 8750 - Train Loss: 0.8630292415618896 - Validation Loss: 1.1461284160614014\n",
      "Epoch 8760 - Train Loss: 0.8623334169387817 - Validation Loss: 1.1438795328140259\n",
      "Epoch 8770 - Train Loss: 0.8573558926582336 - Validation Loss: 1.1415516138076782\n",
      "Epoch 8780 - Train Loss: 0.8562389612197876 - Validation Loss: 1.138816237449646\n",
      "Epoch 8790 - Train Loss: 0.8527149558067322 - Validation Loss: 1.133297085762024\n",
      "Epoch 8800 - Train Loss: 0.8483704924583435 - Validation Loss: 1.1326866149902344\n",
      "Epoch 8810 - Train Loss: 0.8467223644256592 - Validation Loss: 1.1290007829666138\n",
      "Epoch 8820 - Train Loss: 0.8430706262588501 - Validation Loss: 1.1229585409164429\n",
      "Epoch 8830 - Train Loss: 0.841584324836731 - Validation Loss: 1.1223504543304443\n",
      "Epoch 8840 - Train Loss: 0.8382437229156494 - Validation Loss: 1.120617389678955\n",
      "Epoch 8850 - Train Loss: 0.833259105682373 - Validation Loss: 1.1134413480758667\n",
      "Epoch 8860 - Train Loss: 0.8326489925384521 - Validation Loss: 1.1097192764282227\n",
      "Epoch 8870 - Train Loss: 0.8282662630081177 - Validation Loss: 1.1068209409713745\n",
      "Epoch 8880 - Train Loss: 0.8264188170433044 - Validation Loss: 1.1051751375198364\n",
      "Epoch 8890 - Train Loss: 0.8238111138343811 - Validation Loss: 1.1015461683273315\n",
      "Epoch 8900 - Train Loss: 0.8226369619369507 - Validation Loss: 1.1014448404312134\n",
      "Epoch 8910 - Train Loss: 0.8176590800285339 - Validation Loss: 1.094598650932312\n",
      "Epoch 8920 - Train Loss: 0.8165937662124634 - Validation Loss: 1.0943549871444702\n",
      "Epoch 8930 - Train Loss: 0.8143978118896484 - Validation Loss: 1.0900894403457642\n",
      "Epoch 8940 - Train Loss: 0.8125670552253723 - Validation Loss: 1.086424469947815\n",
      "Epoch 8950 - Train Loss: 0.8098293542861938 - Validation Loss: 1.0862945318222046\n",
      "Epoch 8960 - Train Loss: 0.8069757223129272 - Validation Loss: 1.0848034620285034\n",
      "Epoch 8970 - Train Loss: 0.8039008975028992 - Validation Loss: 1.0790741443634033\n",
      "Epoch 8980 - Train Loss: 0.8042029738426208 - Validation Loss: 1.078128695487976\n",
      "Epoch 8990 - Train Loss: 0.7999472618103027 - Validation Loss: 1.0767290592193604\n",
      "Epoch 9000 - Train Loss: 0.7971270680427551 - Validation Loss: 1.0740456581115723\n",
      "Epoch 9010 - Train Loss: 0.7942777276039124 - Validation Loss: 1.0703048706054688\n",
      "Epoch 9020 - Train Loss: 0.7931732535362244 - Validation Loss: 1.0685535669326782\n",
      "Epoch 9030 - Train Loss: 0.7908587455749512 - Validation Loss: 1.0649067163467407\n",
      "Epoch 9040 - Train Loss: 0.7914401888847351 - Validation Loss: 1.0622320175170898\n",
      "Epoch 9050 - Train Loss: 0.7868041396141052 - Validation Loss: 1.0630632638931274\n",
      "Epoch 9060 - Train Loss: 0.7843612432479858 - Validation Loss: 1.0581681728363037\n",
      "Epoch 9070 - Train Loss: 0.7823888659477234 - Validation Loss: 1.0553874969482422\n",
      "Epoch 9080 - Train Loss: 0.7792943716049194 - Validation Loss: 1.0514802932739258\n",
      "Epoch 9090 - Train Loss: 0.7767069935798645 - Validation Loss: 1.0516437292099\n",
      "Epoch 9100 - Train Loss: 0.776068389415741 - Validation Loss: 1.0495022535324097\n",
      "Epoch 9110 - Train Loss: 0.7736667394638062 - Validation Loss: 1.0449374914169312\n",
      "Epoch 9120 - Train Loss: 0.7726354002952576 - Validation Loss: 1.04477858543396\n",
      "Epoch 9130 - Train Loss: 0.7683696746826172 - Validation Loss: 1.0413627624511719\n",
      "Epoch 9140 - Train Loss: 0.7682209014892578 - Validation Loss: 1.0422303676605225\n",
      "Epoch 9150 - Train Loss: 0.765817403793335 - Validation Loss: 1.0388364791870117\n",
      "Epoch 9160 - Train Loss: 0.7632021903991699 - Validation Loss: 1.0360493659973145\n",
      "Epoch 9170 - Train Loss: 0.764241099357605 - Validation Loss: 1.037764072418213\n",
      "Epoch 9180 - Train Loss: 0.7591584920883179 - Validation Loss: 1.030500054359436\n",
      "Epoch 9190 - Train Loss: 0.7601514458656311 - Validation Loss: 1.0290186405181885\n",
      "Epoch 9200 - Train Loss: 0.7557092905044556 - Validation Loss: 1.0286588668823242\n",
      "Epoch 9210 - Train Loss: 0.754107654094696 - Validation Loss: 1.0256117582321167\n",
      "Epoch 9220 - Train Loss: 0.7538958191871643 - Validation Loss: 1.0249359607696533\n",
      "Epoch 9230 - Train Loss: 0.7510281205177307 - Validation Loss: 1.0227559804916382\n",
      "Epoch 9240 - Train Loss: 0.7480286359786987 - Validation Loss: 1.0185530185699463\n",
      "Epoch 9250 - Train Loss: 0.7496646046638489 - Validation Loss: 1.0183454751968384\n",
      "Epoch 9260 - Train Loss: 0.7442947030067444 - Validation Loss: 1.015649676322937\n",
      "Epoch 9270 - Train Loss: 0.7437792420387268 - Validation Loss: 1.0142037868499756\n",
      "Epoch 9280 - Train Loss: 0.7412165403366089 - Validation Loss: 1.0094727277755737\n",
      "Epoch 9290 - Train Loss: 0.7388845086097717 - Validation Loss: 1.0067551136016846\n",
      "Epoch 9300 - Train Loss: 0.7411049008369446 - Validation Loss: 1.008519172668457\n",
      "Epoch 9310 - Train Loss: 0.735382080078125 - Validation Loss: 1.0047091245651245\n",
      "Epoch 9320 - Train Loss: 0.7339180707931519 - Validation Loss: 1.0011416673660278\n",
      "Epoch 9330 - Train Loss: 0.7340253591537476 - Validation Loss: 0.9996017217636108\n",
      "Epoch 9340 - Train Loss: 0.729974627494812 - Validation Loss: 0.998818576335907\n",
      "Epoch 9350 - Train Loss: 0.7285158634185791 - Validation Loss: 0.9957494139671326\n",
      "Epoch 9360 - Train Loss: 0.727325975894928 - Validation Loss: 0.9924416542053223\n",
      "Epoch 9370 - Train Loss: 0.7257881760597229 - Validation Loss: 0.9905076622962952\n",
      "Epoch 9380 - Train Loss: 0.7273213863372803 - Validation Loss: 0.9926041960716248\n",
      "Epoch 9390 - Train Loss: 0.7229759097099304 - Validation Loss: 0.9869552850723267\n",
      "Epoch 9400 - Train Loss: 0.7195320725440979 - Validation Loss: 0.9854938983917236\n",
      "Epoch 9410 - Train Loss: 0.7204902172088623 - Validation Loss: 0.987694501876831\n",
      "Epoch 9420 - Train Loss: 0.7224168181419373 - Validation Loss: 0.9875591397285461\n",
      "Epoch 9430 - Train Loss: 0.7157868146896362 - Validation Loss: 0.9817318916320801\n",
      "Epoch 9440 - Train Loss: 0.7184650897979736 - Validation Loss: 0.9804407954216003\n",
      "Epoch 9450 - Train Loss: 0.7122464776039124 - Validation Loss: 0.9785037040710449\n",
      "Epoch 9460 - Train Loss: 0.7107011675834656 - Validation Loss: 0.9751532673835754\n",
      "Epoch 9470 - Train Loss: 0.7092490196228027 - Validation Loss: 0.9717101454734802\n",
      "Epoch 9480 - Train Loss: 0.7142269611358643 - Validation Loss: 0.9772874116897583\n",
      "Epoch 9490 - Train Loss: 0.7059673070907593 - Validation Loss: 0.971239447593689\n",
      "Epoch 9500 - Train Loss: 0.7043479084968567 - Validation Loss: 0.9667495489120483\n",
      "Epoch 9510 - Train Loss: 0.7050026059150696 - Validation Loss: 0.966644287109375\n",
      "Epoch 9520 - Train Loss: 0.7010972499847412 - Validation Loss: 0.96528160572052\n",
      "Epoch 9530 - Train Loss: 0.7009508013725281 - Validation Loss: 0.9642399549484253\n",
      "Epoch 9540 - Train Loss: 0.6996347904205322 - Validation Loss: 0.9615552425384521\n",
      "Epoch 9550 - Train Loss: 0.6974838376045227 - Validation Loss: 0.960345447063446\n",
      "Epoch 9560 - Train Loss: 0.6979150772094727 - Validation Loss: 0.9607577919960022\n",
      "Epoch 9570 - Train Loss: 0.6947755813598633 - Validation Loss: 0.9559785723686218\n",
      "Epoch 9580 - Train Loss: 0.693683385848999 - Validation Loss: 0.9552679061889648\n",
      "Epoch 9590 - Train Loss: 0.6934168338775635 - Validation Loss: 0.9559919834136963\n",
      "Epoch 9600 - Train Loss: 0.6899533271789551 - Validation Loss: 0.9507283568382263\n",
      "Epoch 9610 - Train Loss: 0.6897451281547546 - Validation Loss: 0.9483776092529297\n",
      "Epoch 9620 - Train Loss: 0.6915857791900635 - Validation Loss: 0.9516837000846863\n",
      "Epoch 9630 - Train Loss: 0.6860470175743103 - Validation Loss: 0.947428822517395\n",
      "Epoch 9640 - Train Loss: 0.6856451630592346 - Validation Loss: 0.9468633532524109\n",
      "Epoch 9650 - Train Loss: 0.6846944689750671 - Validation Loss: 0.9430896043777466\n",
      "Epoch 9660 - Train Loss: 0.6805874109268188 - Validation Loss: 0.9403678774833679\n",
      "Epoch 9670 - Train Loss: 0.6801989078521729 - Validation Loss: 0.9405651092529297\n",
      "Epoch 9680 - Train Loss: 0.6768608689308167 - Validation Loss: 0.935706615447998\n",
      "Epoch 9690 - Train Loss: 0.6792835593223572 - Validation Loss: 0.9363130331039429\n",
      "Epoch 9700 - Train Loss: 0.6765273213386536 - Validation Loss: 0.9375\n",
      "Epoch 9710 - Train Loss: 0.6725173592567444 - Validation Loss: 0.930815577507019\n",
      "Epoch 9720 - Train Loss: 0.6735944151878357 - Validation Loss: 0.930838942527771\n",
      "Epoch 9730 - Train Loss: 0.6727797389030457 - Validation Loss: 0.9313916563987732\n",
      "Epoch 9740 - Train Loss: 0.670038104057312 - Validation Loss: 0.9276469945907593\n",
      "Epoch 9750 - Train Loss: 0.6672436594963074 - Validation Loss: 0.9266930222511292\n",
      "Epoch 9760 - Train Loss: 0.6647976040840149 - Validation Loss: 0.9231789708137512\n",
      "Epoch 9770 - Train Loss: 0.6661742925643921 - Validation Loss: 0.9244095087051392\n",
      "Epoch 9780 - Train Loss: 0.6638404130935669 - Validation Loss: 0.9208922386169434\n",
      "Epoch 9790 - Train Loss: 0.6641398072242737 - Validation Loss: 0.9214240312576294\n",
      "Epoch 9800 - Train Loss: 0.6585966944694519 - Validation Loss: 0.9171129465103149\n",
      "Epoch 9810 - Train Loss: 0.6589182019233704 - Validation Loss: 0.9155216813087463\n",
      "Epoch 9820 - Train Loss: 0.6575021147727966 - Validation Loss: 0.9136824011802673\n",
      "Epoch 9830 - Train Loss: 0.6542037129402161 - Validation Loss: 0.912137508392334\n",
      "Epoch 9840 - Train Loss: 0.6568814516067505 - Validation Loss: 0.9149881601333618\n",
      "Epoch 9850 - Train Loss: 0.653237521648407 - Validation Loss: 0.9092523455619812\n",
      "Epoch 9860 - Train Loss: 0.6498534083366394 - Validation Loss: 0.9071630835533142\n",
      "Epoch 9870 - Train Loss: 0.6534264087677002 - Validation Loss: 0.9102326035499573\n",
      "Epoch 9880 - Train Loss: 0.6489596962928772 - Validation Loss: 0.9042582511901855\n",
      "Epoch 9890 - Train Loss: 0.6479372382164001 - Validation Loss: 0.904998242855072\n",
      "Epoch 9900 - Train Loss: 0.6463614106178284 - Validation Loss: 0.9023824334144592\n",
      "Epoch 9910 - Train Loss: 0.6428393125534058 - Validation Loss: 0.9001743197441101\n",
      "Epoch 9920 - Train Loss: 0.6526158452033997 - Validation Loss: 0.9081683158874512\n",
      "Epoch 9930 - Train Loss: 0.642763614654541 - Validation Loss: 0.9011346101760864\n",
      "Epoch 9940 - Train Loss: 0.638565182685852 - Validation Loss: 0.8956184983253479\n",
      "Epoch 9950 - Train Loss: 0.6397414803504944 - Validation Loss: 0.8956789970397949\n",
      "Epoch 9960 - Train Loss: 0.6365265846252441 - Validation Loss: 0.8937719464302063\n",
      "Epoch 9970 - Train Loss: 0.6369448900222778 - Validation Loss: 0.8929713368415833\n",
      "Epoch 9980 - Train Loss: 0.6340685486793518 - Validation Loss: 0.8902746438980103\n",
      "Epoch 9990 - Train Loss: 0.6338819861412048 - Validation Loss: 0.8889126777648926\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.8876019716262817\n"
     ]
    }
   ],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "model = CO2PredictionModel(X_train_tensor.shape[1])\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
    "\n",
    "loss = +np.inf\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "# Training loop\n",
    "epochs = 10000\n",
    "epoch = 0\n",
    "\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', patience=20, verbose=True)\n",
    "\n",
    "best_loss = float('inf')\n",
    "patience = 0\n",
    "\n",
    "while patience < 20 and epoch < epochs:    \n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train_tensor)\n",
    "    loss = criterion(outputs, y_train_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Model evalutaion on validation set\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        val_outputs = model(X_test_tensor)\n",
    "        val_loss = criterion(val_outputs, y_test_tensor)\n",
    "    \n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    # Store training and validation losses\n",
    "    train_losses.append(loss.item())\n",
    "    val_losses.append(val_loss.item())\n",
    "\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        patience = 0\n",
    "    else:\n",
    "        patience += 1\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch} - Train Loss: {loss.item()} - Validation Loss: {val_loss.item()}\")\n",
    "        \n",
    "    epoch += 1\n",
    "\n",
    "# plot in logarithmic scale\n",
    "plt.plot(np.log(train_losses), label='train loss')\n",
    "plt.plot(np.log(val_losses), label='validation loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Model Evaluation on Test Set\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    test_outputs = model(X_test_tensor)\n",
    "    test_loss = criterion(test_outputs, y_test_tensor)\n",
    "    print(\"Test Loss:\", test_loss.item())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La dernière étape consiste à effectuer les prédictions sur les données de test et à évaluer la performance du modèle en utilisant la méthode `mean_squared_error` de la librairie `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('insa-ml-2024-regression/test.csv')\n",
    "\n",
    "test_data.fillna(0, inplace=True)\n",
    "\n",
    "test_labelencoder = {}\n",
    "\n",
    "for col in categorical_cols:\n",
    "    test_labelencoder[col] = LabelEncoder()\n",
    "    test_data[col] = test_labelencoder[col].fit_transform(test_data[col])\n",
    "    \n",
    "    \n",
    "X_test_data = test_data[X.columns]\n",
    "\n",
    "X_test_data_selected = selector.transform(X_test_data)\n",
    "X_test_data_selected = scaler.transform(X_test_data_selected)\n",
    "\n",
    "X_test_data_tensor = torch.tensor(X_test_data_selected, dtype=torch.float32)\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_pred_tensor = model(X_test_data_tensor)\n",
    "    y_pred = y_pred_tensor.numpy()\n",
    "\n",
    "# save the predictions to a csv file\n",
    "submission = pd.DataFrame({'id': test_data['id'], 'co2': y_pred.flatten()})\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
