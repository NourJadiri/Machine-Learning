{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_regression, f_regression, f_classif, mutual_info_classif\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "# load the dataset\n",
    "data = pd.read_csv('insa-ml-2024-regression/train.csv')\n",
    "\n",
    "data.drop(columns=['hc', 'id'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On remarque qu'il y a quelques valeurs de la colonne hcnox qui manquent (nan), lais que cette variable présente une forte correlation avec le fuel_type. On peut donc essayer de prédire les valeurs manquantes de hcnox en fonction du fuel_type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_hcnox_by_fuel_type = data.groupby('fuel_type')['hcnox'].median()\n",
    "\n",
    "def fill_hcnox(row):\n",
    "    if pd.isna(row['hcnox']):\n",
    "        return median_hcnox_by_fuel_type[row['fuel_type']]\n",
    "    else:\n",
    "        return row['hcnox']\n",
    "\n",
    "data['hcnox'] = data.apply(fill_hcnox, axis=1)\n",
    "\n",
    "# delete rows where urban_cons is nan\n",
    "data.dropna(subset=['urb_cons', 'exturb_cons'], inplace=True)\n",
    "\n",
    "# replace nan values of hcnox by 0 if data['fuel_type'] contains the string 'EH' or 'ES' \n",
    "data.loc[data['fuel_type'].str.contains('EH|ES|GN'), 'hcnox'] = data.loc[data['fuel_type'].str.contains('EH|ES'), 'hcnox'].fillna(0)\n",
    "\n",
    "# same thing with nox and co\n",
    "data.loc[data['fuel_type'].str.contains('EH|ES'), 'nox'] = data.loc[data['fuel_type'].str.contains('EH|ES'), 'nox'].fillna(0)\n",
    "data.loc[data['fuel_type'].str.contains('EH|ES'), 'co'] = data.loc[data['fuel_type'].str.contains('EH|ES'), 'co'].fillna(0)\n",
    "\n",
    "# delete all rows that have nan values for co or nox but not containing 'EH' or 'ES' in fuel_type\n",
    "data.dropna(subset=['nox', 'co'], inplace=True)\n",
    "\n",
    "# replace all nan values in the dataframe by 0\n",
    "data.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On encode ensuite les colonnes qui ont des valeurs non numériques. La librairie sklearn permet de faire cela facilement. On encode les colonnes qui ont des valeurs non numériques en utilisant la méthode `LabelEncoder` de la librairie `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding categorical data\n",
    "labelencoder = {}\n",
    "\n",
    "categorical_cols = ['brand', 'model','car_class','range','fuel_type','hybrid','max_power','grbx_type_ratios']\n",
    "for col in categorical_cols:\n",
    "    labelencoder[col] = LabelEncoder()\n",
    "    data[col] = labelencoder[col].fit_transform(data[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La prochaine étape ensuite consistera en le choix des features qui sont intéressantes à considérer dans notre modèle.\n",
    "Dans un premier temps, on prend toutes les features qui sont numériques. On utilise la méthode `SelectKBest` de la librairie `sklearn` pour sélectionner les features qui sont les plus importantes. On utilise ensuite la méthode `fit_transform` pour transformer les données en utilisant les features sélectionnées.\n",
    "\n",
    "Les émissions de co2 étant l'objectif de notre étude, on isole la colonne afin selectionner les features pertinentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(columns=['co2'])\n",
    "y = data['co2']\n",
    "\n",
    "selector = SelectKBest(score_func= f_classif, k = 16)\n",
    "\n",
    "X_selected = selector.fit_transform(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuite on sépare les données en données d'entraînement et données de test. On utilise la méthode `train_test_split` de la librairie `sklearn` pour cela. On met le paramètre random_state à 0 pour s'assurer d'avoir des résultats reproductibles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_learn, X_test, y_learn, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_learn = scaler.fit_transform(X_learn)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Etant donné l'utilisation de pytorch, il faut convertir les données en tenseurs. On utilise la méthode `torch.tensor` pour cela."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = torch.tensor(X_learn, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_learn.values, dtype=torch.float32).view(-1, 1)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).view(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il est temps de créer notre modèle de regression :D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CO2PredictionModel(nn.Module):\n",
    "    def __init__(self, input_dim) -> None:\n",
    "        super(CO2PredictionModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 64)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.bn2 = nn.BatchNorm1d(32)\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La dernière étape consiste à entrainer le modèle sur les données de training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "model = CO2PredictionModel(X_train_tensor.shape[1])\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
    "\n",
    "loss = +np.inf\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "# Training loop\n",
    "epochs = 10000\n",
    "epoch = 0\n",
    "\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', patience=20, verbose=True)\n",
    "\n",
    "best_loss = float('inf')\n",
    "patience = 0\n",
    "\n",
    "while patience < 20 and epoch < epochs:    \n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train_tensor)\n",
    "    loss = criterion(outputs, y_train_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Model evalutaion on validation set\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        val_outputs = model(X_test_tensor)\n",
    "        val_loss = criterion(val_outputs, y_test_tensor)\n",
    "    \n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    # Store training and validation losses\n",
    "    train_losses.append(loss.item())\n",
    "    val_losses.append(val_loss.item())\n",
    "\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        patience = 0\n",
    "    else:\n",
    "        patience += 1\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch} - Train Loss: {loss.item()} - Validation Loss: {val_loss.item()}\")\n",
    "        \n",
    "    epoch += 1\n",
    "\n",
    "# plot in logarithmic scale\n",
    "plt.plot(np.log(train_losses), label='train loss')\n",
    "plt.plot(np.log(val_losses), label='validation loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Model Evaluation on Test Set\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    test_outputs = model(X_test_tensor)\n",
    "    test_loss = criterion(test_outputs, y_test_tensor)\n",
    "    print(\"Test Loss:\", test_loss.item())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La dernière étape consiste à effectuer les prédictions sur les données de test et à évaluer la performance du modèle en utilisant la méthode `mean_squared_error` de la librairie `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('insa-ml-2024-regression/test.csv')\n",
    "\n",
    "test_data.fillna(0, inplace=True)\n",
    "\n",
    "test_labelencoder = {}\n",
    "\n",
    "for col in categorical_cols:\n",
    "    test_labelencoder[col] = LabelEncoder()\n",
    "    test_data[col] = test_labelencoder[col].fit_transform(test_data[col])\n",
    "    \n",
    "    \n",
    "X_test_data = test_data[X.columns]\n",
    "\n",
    "X_test_data_selected = selector.transform(X_test_data)\n",
    "X_test_data_selected = scaler.transform(X_test_data_selected)\n",
    "\n",
    "X_test_data_tensor = torch.tensor(X_test_data_selected, dtype=torch.float32)\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_pred_tensor = model(X_test_data_tensor)\n",
    "    y_pred = y_pred_tensor.numpy()\n",
    "\n",
    "# save the predictions to a csv file\n",
    "submission = pd.DataFrame({'id': test_data['id'], 'co2': y_pred.flatten()})\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
